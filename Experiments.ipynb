{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350c987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns\n",
    "import time\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "\n",
    "# Set a specific random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "942988b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d370a0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2060'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "x = x.to(device)\n",
    "print(x.device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da039856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0a233",
   "metadata": {},
   "source": [
    "# loading the DS002 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0082dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Operation time (min):  0.30052083333333335\n",
      "\n",
      "W shape: (6517190, 4)\n",
      "X_s shape: (6517190, 14)\n",
      "X_v shape: (6517190, 14)\n",
      "T shape: (6517190, 10)\n",
      "A shape: (6517190, 4)\n"
     ]
    }
   ],
   "source": [
    "# Time tracking, Operation time (min):  0.003\n",
    "filename=\"N-CMAPSS_DS02-006.h5\"\n",
    "t = time.process_time()  \n",
    "\n",
    "# Load data\n",
    "with h5py.File(filename, 'r') as hdf:\n",
    "        # Development set\n",
    "        W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "        X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "        X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "        A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        W_test = np.array(hdf.get('W_test'))           # W\n",
    "        X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "        X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "        T_test = np.array(hdf.get('T_test'))           # T\n",
    "        Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "        A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "        \n",
    "        # Varnams\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        # from np.array to list dtype U4/U5\n",
    "        W_var = list(np.array(W_var, dtype='U20'))\n",
    "        X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "        X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "        T_var = list(np.array(T_var, dtype='U20'))\n",
    "        A_var = list(np.array(A_var, dtype='U20'))\n",
    "                          \n",
    "W = np.concatenate((W_dev, W_test), axis=0)  \n",
    "X_s = np.concatenate((X_s_dev, X_s_test), axis=0)\n",
    "X_v = np.concatenate((X_v_dev, X_v_test), axis=0)\n",
    "T = np.concatenate((T_dev, T_test), axis=0)\n",
    "Y = np.concatenate((Y_dev, Y_test), axis=0) \n",
    "A = np.concatenate((A_dev, A_test), axis=0) \n",
    "    \n",
    "print('')\n",
    "print(\"Operation time (min): \" , (time.process_time()-t)/60)\n",
    "print('')\n",
    "print (\"W shape: \" + str(W.shape))\n",
    "print (\"X_s shape: \" + str(X_s.shape))\n",
    "print (\"X_v shape: \" + str(X_v.shape))\n",
    "print (\"T shape: \" + str(T.shape))\n",
    "print (\"A shape: \" + str(A.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2fb4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W = DataFrame(data=W, columns=W_var)\n",
    "df_X_s_parameters = DataFrame(data=X_s, columns=X_s_var)\n",
    "df_X_v_parameters = DataFrame(data=X_v, columns=X_v_var)\n",
    "df_T = DataFrame(data=T, columns=T_var)\n",
    "df_Y = DataFrame(data=Y, columns=['RUL'])\n",
    "df_A = DataFrame(data=A, columns=A_var)\n",
    "df_unit1=pd.concat([df_A,df_X_s_parameters,df_X_v_parameters,df_T,df_Y],axis=1)\n",
    "def prepare_data(dataframe,list_of_columns_to_drop):\n",
    "    dataframe=dataframe.drop(columns=list_of_columns_to_drop)\n",
    "    return dataframe\n",
    "def data_test_train_splitter(DataFrame,train_idx,test_idx,list_of_columns_to_drop):\n",
    "    train_df=prepare_data(DataFrame.loc[DataFrame[\"unit\"].isin(train_idx)],list_of_columns_to_drop)\n",
    "    test_df=prepare_data(DataFrame.loc[DataFrame[\"unit\"].isin(test_idx)],list_of_columns_to_drop)\n",
    "    return (train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18e4a311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206.38333333333333"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12383/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00b6e3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit</th>\n",
       "      <th>cycle</th>\n",
       "      <th>Fc</th>\n",
       "      <th>hs</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>P2</th>\n",
       "      <th>...</th>\n",
       "      <th>fan_flow_mod</th>\n",
       "      <th>LPC_eff_mod</th>\n",
       "      <th>LPC_flow_mod</th>\n",
       "      <th>HPC_eff_mod</th>\n",
       "      <th>HPC_flow_mod</th>\n",
       "      <th>HPT_eff_mod</th>\n",
       "      <th>HPT_flow_mod</th>\n",
       "      <th>LPT_eff_mod</th>\n",
       "      <th>LPT_flow_mod</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>895356</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>605.415527</td>\n",
       "      <td>1449.491663</td>\n",
       "      <td>1833.549522</td>\n",
       "      <td>1234.221535</td>\n",
       "      <td>16.231743</td>\n",
       "      <td>11.859551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895357</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>605.390722</td>\n",
       "      <td>1449.436713</td>\n",
       "      <td>1833.351521</td>\n",
       "      <td>1234.055132</td>\n",
       "      <td>16.228119</td>\n",
       "      <td>11.856384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895358</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>605.375621</td>\n",
       "      <td>1449.396643</td>\n",
       "      <td>1833.357153</td>\n",
       "      <td>1234.066198</td>\n",
       "      <td>16.225811</td>\n",
       "      <td>11.854989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895359</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>605.368840</td>\n",
       "      <td>1449.372692</td>\n",
       "      <td>1833.307833</td>\n",
       "      <td>1234.018671</td>\n",
       "      <td>16.224054</td>\n",
       "      <td>11.853761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895360</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>605.434935</td>\n",
       "      <td>1449.462974</td>\n",
       "      <td>1833.493910</td>\n",
       "      <td>1234.119184</td>\n",
       "      <td>16.229294</td>\n",
       "      <td>11.858560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907734</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>549.717298</td>\n",
       "      <td>1212.366823</td>\n",
       "      <td>1395.886472</td>\n",
       "      <td>1050.282698</td>\n",
       "      <td>13.302806</td>\n",
       "      <td>11.616100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907735</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>549.747191</td>\n",
       "      <td>1212.411133</td>\n",
       "      <td>1395.766625</td>\n",
       "      <td>1050.196906</td>\n",
       "      <td>13.306581</td>\n",
       "      <td>11.619138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907736</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>549.721489</td>\n",
       "      <td>1212.413239</td>\n",
       "      <td>1395.790805</td>\n",
       "      <td>1050.257089</td>\n",
       "      <td>13.305935</td>\n",
       "      <td>11.617879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907737</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>549.801230</td>\n",
       "      <td>1212.557861</td>\n",
       "      <td>1396.142076</td>\n",
       "      <td>1050.504337</td>\n",
       "      <td>13.314397</td>\n",
       "      <td>11.626024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907738</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>549.829147</td>\n",
       "      <td>1212.602076</td>\n",
       "      <td>1395.998661</td>\n",
       "      <td>1050.386384</td>\n",
       "      <td>13.317720</td>\n",
       "      <td>11.628628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12383 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        unit  cycle   Fc   hs         T24          T30          T48  \\\n",
       "895356   5.0    5.0  3.0  1.0  605.415527  1449.491663  1833.549522   \n",
       "895357   5.0    5.0  3.0  1.0  605.390722  1449.436713  1833.351521   \n",
       "895358   5.0    5.0  3.0  1.0  605.375621  1449.396643  1833.357153   \n",
       "895359   5.0    5.0  3.0  1.0  605.368840  1449.372692  1833.307833   \n",
       "895360   5.0    5.0  3.0  1.0  605.434935  1449.462974  1833.493910   \n",
       "...      ...    ...  ...  ...         ...          ...          ...   \n",
       "907734   5.0    5.0  3.0  1.0  549.717298  1212.366823  1395.886472   \n",
       "907735   5.0    5.0  3.0  1.0  549.747191  1212.411133  1395.766625   \n",
       "907736   5.0    5.0  3.0  1.0  549.721489  1212.413239  1395.790805   \n",
       "907737   5.0    5.0  3.0  1.0  549.801230  1212.557861  1396.142076   \n",
       "907738   5.0    5.0  3.0  1.0  549.829147  1212.602076  1395.998661   \n",
       "\n",
       "                T50        P15         P2  ...  fan_flow_mod  LPC_eff_mod  \\\n",
       "895356  1234.221535  16.231743  11.859551  ...           0.0          0.0   \n",
       "895357  1234.055132  16.228119  11.856384  ...           0.0          0.0   \n",
       "895358  1234.066198  16.225811  11.854989  ...           0.0          0.0   \n",
       "895359  1234.018671  16.224054  11.853761  ...           0.0          0.0   \n",
       "895360  1234.119184  16.229294  11.858560  ...           0.0          0.0   \n",
       "...             ...        ...        ...  ...           ...          ...   \n",
       "907734  1050.282698  13.302806  11.616100  ...           0.0          0.0   \n",
       "907735  1050.196906  13.306581  11.619138  ...           0.0          0.0   \n",
       "907736  1050.257089  13.305935  11.617879  ...           0.0          0.0   \n",
       "907737  1050.504337  13.314397  11.626024  ...           0.0          0.0   \n",
       "907738  1050.386384  13.317720  11.628628  ...           0.0          0.0   \n",
       "\n",
       "        LPC_flow_mod  HPC_eff_mod  HPC_flow_mod  HPT_eff_mod  HPT_flow_mod  \\\n",
       "895356           0.0          0.0           0.0    -0.000755           0.0   \n",
       "895357           0.0          0.0           0.0    -0.000755           0.0   \n",
       "895358           0.0          0.0           0.0    -0.000755           0.0   \n",
       "895359           0.0          0.0           0.0    -0.000755           0.0   \n",
       "895360           0.0          0.0           0.0    -0.000755           0.0   \n",
       "...              ...          ...           ...          ...           ...   \n",
       "907734           0.0          0.0           0.0    -0.000755           0.0   \n",
       "907735           0.0          0.0           0.0    -0.000755           0.0   \n",
       "907736           0.0          0.0           0.0    -0.000755           0.0   \n",
       "907737           0.0          0.0           0.0    -0.000755           0.0   \n",
       "907738           0.0          0.0           0.0    -0.000755           0.0   \n",
       "\n",
       "        LPT_eff_mod  LPT_flow_mod  RUL  \n",
       "895356          0.0           0.0   84  \n",
       "895357          0.0           0.0   84  \n",
       "895358          0.0           0.0   84  \n",
       "895359          0.0           0.0   84  \n",
       "895360          0.0           0.0   84  \n",
       "...             ...           ...  ...  \n",
       "907734          0.0           0.0   84  \n",
       "907735          0.0           0.0   84  \n",
       "907736          0.0           0.0   84  \n",
       "907737          0.0           0.0   84  \n",
       "907738          0.0           0.0   84  \n",
       "\n",
       "[12383 rows x 43 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unit1.loc[(df_unit1[\"unit\"]==5) & (df_unit1[\"cycle\"]==5) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b75050f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit</th>\n",
       "      <th>Fc</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>P2</th>\n",
       "      <th>P21</th>\n",
       "      <th>P24</th>\n",
       "      <th>...</th>\n",
       "      <th>W31</th>\n",
       "      <th>W32</th>\n",
       "      <th>W48</th>\n",
       "      <th>W50</th>\n",
       "      <th>SmFan</th>\n",
       "      <th>SmLPC</th>\n",
       "      <th>SmHPC</th>\n",
       "      <th>phi</th>\n",
       "      <th>HPT_eff_mod</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>600.148034</td>\n",
       "      <td>1438.498187</td>\n",
       "      <td>1818.027714</td>\n",
       "      <td>1228.129848</td>\n",
       "      <td>15.806267</td>\n",
       "      <td>11.577097</td>\n",
       "      <td>16.046971</td>\n",
       "      <td>20.126624</td>\n",
       "      <td>...</td>\n",
       "      <td>26.498785</td>\n",
       "      <td>15.899271</td>\n",
       "      <td>215.844851</td>\n",
       "      <td>228.411666</td>\n",
       "      <td>16.648833</td>\n",
       "      <td>9.898130</td>\n",
       "      <td>25.376144</td>\n",
       "      <td>41.893990</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>600.055894</td>\n",
       "      <td>1438.350208</td>\n",
       "      <td>1817.682618</td>\n",
       "      <td>1227.879113</td>\n",
       "      <td>15.795477</td>\n",
       "      <td>11.568235</td>\n",
       "      <td>16.036017</td>\n",
       "      <td>20.113218</td>\n",
       "      <td>...</td>\n",
       "      <td>26.486552</td>\n",
       "      <td>15.891931</td>\n",
       "      <td>215.745634</td>\n",
       "      <td>228.307014</td>\n",
       "      <td>16.639222</td>\n",
       "      <td>9.904927</td>\n",
       "      <td>25.380549</td>\n",
       "      <td>41.884434</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>600.210756</td>\n",
       "      <td>1439.109101</td>\n",
       "      <td>1820.020627</td>\n",
       "      <td>1229.422522</td>\n",
       "      <td>15.807747</td>\n",
       "      <td>11.574866</td>\n",
       "      <td>16.048474</td>\n",
       "      <td>20.130956</td>\n",
       "      <td>...</td>\n",
       "      <td>26.519340</td>\n",
       "      <td>15.911604</td>\n",
       "      <td>216.019054</td>\n",
       "      <td>228.592279</td>\n",
       "      <td>16.649823</td>\n",
       "      <td>9.923503</td>\n",
       "      <td>25.318848</td>\n",
       "      <td>41.953848</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>600.369717</td>\n",
       "      <td>1439.240230</td>\n",
       "      <td>1819.188327</td>\n",
       "      <td>1228.538726</td>\n",
       "      <td>15.816360</td>\n",
       "      <td>11.578198</td>\n",
       "      <td>16.057218</td>\n",
       "      <td>20.146716</td>\n",
       "      <td>...</td>\n",
       "      <td>26.532044</td>\n",
       "      <td>15.919226</td>\n",
       "      <td>216.121238</td>\n",
       "      <td>228.702994</td>\n",
       "      <td>16.653812</td>\n",
       "      <td>9.905518</td>\n",
       "      <td>25.361981</td>\n",
       "      <td>41.914342</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>600.298227</td>\n",
       "      <td>1439.064004</td>\n",
       "      <td>1818.963540</td>\n",
       "      <td>1228.389046</td>\n",
       "      <td>15.807513</td>\n",
       "      <td>11.571593</td>\n",
       "      <td>16.048236</td>\n",
       "      <td>20.135888</td>\n",
       "      <td>...</td>\n",
       "      <td>26.518460</td>\n",
       "      <td>15.911076</td>\n",
       "      <td>216.008509</td>\n",
       "      <td>228.584788</td>\n",
       "      <td>16.649031</td>\n",
       "      <td>9.897465</td>\n",
       "      <td>25.363994</td>\n",
       "      <td>41.911503</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853137</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>549.873954</td>\n",
       "      <td>1228.114477</td>\n",
       "      <td>1470.594274</td>\n",
       "      <td>1096.724266</td>\n",
       "      <td>13.116898</td>\n",
       "      <td>11.199832</td>\n",
       "      <td>13.316648</td>\n",
       "      <td>15.164883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.738961</td>\n",
       "      <td>10.043377</td>\n",
       "      <td>135.696974</td>\n",
       "      <td>143.719571</td>\n",
       "      <td>16.959057</td>\n",
       "      <td>6.783033</td>\n",
       "      <td>31.180818</td>\n",
       "      <td>34.338442</td>\n",
       "      <td>-0.018145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853138</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>549.907672</td>\n",
       "      <td>1228.177220</td>\n",
       "      <td>1470.739956</td>\n",
       "      <td>1096.830687</td>\n",
       "      <td>13.120695</td>\n",
       "      <td>11.203387</td>\n",
       "      <td>13.320503</td>\n",
       "      <td>15.169186</td>\n",
       "      <td>...</td>\n",
       "      <td>16.742474</td>\n",
       "      <td>10.045484</td>\n",
       "      <td>135.725237</td>\n",
       "      <td>143.749289</td>\n",
       "      <td>16.964797</td>\n",
       "      <td>6.782442</td>\n",
       "      <td>31.176976</td>\n",
       "      <td>34.342768</td>\n",
       "      <td>-0.018145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853139</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>549.923591</td>\n",
       "      <td>1228.209936</td>\n",
       "      <td>1470.703869</td>\n",
       "      <td>1096.792601</td>\n",
       "      <td>13.122416</td>\n",
       "      <td>11.204765</td>\n",
       "      <td>13.322250</td>\n",
       "      <td>15.171310</td>\n",
       "      <td>...</td>\n",
       "      <td>16.744940</td>\n",
       "      <td>10.046964</td>\n",
       "      <td>135.745765</td>\n",
       "      <td>143.771001</td>\n",
       "      <td>16.966299</td>\n",
       "      <td>6.782323</td>\n",
       "      <td>31.179834</td>\n",
       "      <td>34.339757</td>\n",
       "      <td>-0.018145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853140</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>549.947285</td>\n",
       "      <td>1228.250165</td>\n",
       "      <td>1470.788949</td>\n",
       "      <td>1096.864006</td>\n",
       "      <td>13.125276</td>\n",
       "      <td>11.207342</td>\n",
       "      <td>13.325154</td>\n",
       "      <td>15.174606</td>\n",
       "      <td>...</td>\n",
       "      <td>16.747638</td>\n",
       "      <td>10.048583</td>\n",
       "      <td>135.767263</td>\n",
       "      <td>143.793778</td>\n",
       "      <td>16.967980</td>\n",
       "      <td>6.780795</td>\n",
       "      <td>31.178010</td>\n",
       "      <td>34.342228</td>\n",
       "      <td>-0.018145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853141</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>549.955465</td>\n",
       "      <td>1228.276407</td>\n",
       "      <td>1470.788677</td>\n",
       "      <td>1096.865343</td>\n",
       "      <td>13.126566</td>\n",
       "      <td>11.208277</td>\n",
       "      <td>13.326463</td>\n",
       "      <td>15.176117</td>\n",
       "      <td>...</td>\n",
       "      <td>16.749745</td>\n",
       "      <td>10.049847</td>\n",
       "      <td>135.784825</td>\n",
       "      <td>143.812354</td>\n",
       "      <td>16.964776</td>\n",
       "      <td>6.781634</td>\n",
       "      <td>31.178748</td>\n",
       "      <td>34.341079</td>\n",
       "      <td>-0.018145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>853142 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        unit   Fc         T24          T30          T48          T50  \\\n",
       "0        2.0  3.0  600.148034  1438.498187  1818.027714  1228.129848   \n",
       "1        2.0  3.0  600.055894  1438.350208  1817.682618  1227.879113   \n",
       "2        2.0  3.0  600.210756  1439.109101  1820.020627  1229.422522   \n",
       "3        2.0  3.0  600.369717  1439.240230  1819.188327  1228.538726   \n",
       "4        2.0  3.0  600.298227  1439.064004  1818.963540  1228.389046   \n",
       "...      ...  ...         ...          ...          ...          ...   \n",
       "853137   2.0  3.0  549.873954  1228.114477  1470.594274  1096.724266   \n",
       "853138   2.0  3.0  549.907672  1228.177220  1470.739956  1096.830687   \n",
       "853139   2.0  3.0  549.923591  1228.209936  1470.703869  1096.792601   \n",
       "853140   2.0  3.0  549.947285  1228.250165  1470.788949  1096.864006   \n",
       "853141   2.0  3.0  549.955465  1228.276407  1470.788677  1096.865343   \n",
       "\n",
       "              P15         P2        P21        P24  ...        W31        W32  \\\n",
       "0       15.806267  11.577097  16.046971  20.126624  ...  26.498785  15.899271   \n",
       "1       15.795477  11.568235  16.036017  20.113218  ...  26.486552  15.891931   \n",
       "2       15.807747  11.574866  16.048474  20.130956  ...  26.519340  15.911604   \n",
       "3       15.816360  11.578198  16.057218  20.146716  ...  26.532044  15.919226   \n",
       "4       15.807513  11.571593  16.048236  20.135888  ...  26.518460  15.911076   \n",
       "...           ...        ...        ...        ...  ...        ...        ...   \n",
       "853137  13.116898  11.199832  13.316648  15.164883  ...  16.738961  10.043377   \n",
       "853138  13.120695  11.203387  13.320503  15.169186  ...  16.742474  10.045484   \n",
       "853139  13.122416  11.204765  13.322250  15.171310  ...  16.744940  10.046964   \n",
       "853140  13.125276  11.207342  13.325154  15.174606  ...  16.747638  10.048583   \n",
       "853141  13.126566  11.208277  13.326463  15.176117  ...  16.749745  10.049847   \n",
       "\n",
       "               W48         W50      SmFan     SmLPC      SmHPC        phi  \\\n",
       "0       215.844851  228.411666  16.648833  9.898130  25.376144  41.893990   \n",
       "1       215.745634  228.307014  16.639222  9.904927  25.380549  41.884434   \n",
       "2       216.019054  228.592279  16.649823  9.923503  25.318848  41.953848   \n",
       "3       216.121238  228.702994  16.653812  9.905518  25.361981  41.914342   \n",
       "4       216.008509  228.584788  16.649031  9.897465  25.363994  41.911503   \n",
       "...            ...         ...        ...       ...        ...        ...   \n",
       "853137  135.696974  143.719571  16.959057  6.783033  31.180818  34.338442   \n",
       "853138  135.725237  143.749289  16.964797  6.782442  31.176976  34.342768   \n",
       "853139  135.745765  143.771001  16.966299  6.782323  31.179834  34.339757   \n",
       "853140  135.767263  143.793778  16.967980  6.780795  31.178010  34.342228   \n",
       "853141  135.784825  143.812354  16.964776  6.781634  31.178748  34.341079   \n",
       "\n",
       "        HPT_eff_mod  RUL  \n",
       "0         -0.000638   74  \n",
       "1         -0.000638   74  \n",
       "2         -0.000638   74  \n",
       "3         -0.000638   74  \n",
       "4         -0.000638   74  \n",
       "...             ...  ...  \n",
       "853137    -0.018145    0  \n",
       "853138    -0.018145    0  \n",
       "853139    -0.018145    0  \n",
       "853140    -0.018145    0  \n",
       "853141    -0.018145    0  \n",
       "\n",
       "[853142 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx=[2]\n",
    "test_idx=[14,11,15]\n",
    "list_of_columns_to_drop=[\"cycle\",\"hs\",'fan_eff_mod','fan_flow_mod','LPC_eff_mod','HPC_eff_mod','HPC_flow_mod','HPT_flow_mod','LPT_eff_mod','LPT_flow_mod','LPC_flow_mod']\n",
    "train,test=data_test_train_splitter(df_unit1,train_idx,test_idx,list_of_columns_to_drop)\n",
    "train.drop(columns=[\"unit\",\"Fc\"])\n",
    "test.drop(columns=[\"unit\",\"Fc\"])\n",
    "train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523404f0",
   "metadata": {},
   "source": [
    "### LSTM with Hidden size=32 and CNN channels=32 Loss on all=114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2610c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming df_unit1 is already defined\n",
    "features = train.drop(columns=['RUL',\"unit\",\"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,bidirectional=False)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size + cnn_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        cnn_out = self.conv1d(x.unsqueeze(1))\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2))\n",
    "        cnn_out = cnn_out.squeeze(2)\n",
    "        combined_out = torch.cat((lstm_out, cnn_out), dim=1)\n",
    "        output = self.fc(combined_out)\n",
    "        return output\n",
    "\n",
    "# # Explicitly set the device to CPU\n",
    "# device = torch.device(\"cpu\")\n",
    "# print(f\"Training on device: {device}\")\n",
    "\n",
    "input_size = features_scaled.shape[1] \n",
    "hidden_size = 32 \n",
    "num_layers = 2  \n",
    "cnn_channels = 32 \n",
    "batch_size = 64 \n",
    "learning_rate = 0.001 \n",
    "num_epochs = 10  \n",
    "\n",
    "\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = Model(input_size, hidden_size, num_layers, cnn_channels).to(device)  # Move model to the device\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move data to the device\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "# on the test all print(\"losss= 114.60372838793877\")\n",
    "# on 14 ,,35.01807977637466"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76edb4c5",
   "metadata": {},
   "source": [
    "# BiLSTM with hidden=64 and CNN channels=32 Loss=84 you get 76 if you only use unit 2 for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b995a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 7.2884\n",
      "Epoch [2/10], Loss: 4.3562\n",
      "Epoch [3/10], Loss: 3.2852\n",
      "Epoch [4/10], Loss: 4.4299\n",
      "Epoch [5/10], Loss: 1.6721\n",
      "Epoch [6/10], Loss: 0.1865\n",
      "Epoch [7/10], Loss: 2.9905\n",
      "Epoch [8/10], Loss: 0.1893\n",
      "Epoch [9/10], Loss: 0.6834\n",
      "Epoch [10/10], Loss: 0.2889\n",
      "Training time: 509.66 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming df_unit1 is already defined\n",
    "features = train.drop(columns=['RUL', \"unit\", \"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension if needed\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size * 2 + cnn_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(f\"Input shape: {x.shape}\")\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "#         print(f\"LSTM output shape: {lstm_out.shape}\")\n",
    "        # Use the output from the last time step for LSTM output\n",
    "        lstm_out = lstm_out[:, -1, :]  # Shape: (batch_size, hidden_size * 2)\n",
    "#         print(f\"LSTM last time step output shape: {lstm_out.shape}\")\n",
    "\n",
    "        # CNN expects input of shape (batch_size, in_channels, seq_len)\n",
    "        cnn_in = x.squeeze(1).unsqueeze(1)  # Shape: (batch_size, 1, seq_len)\n",
    "#         print(f\"CNN input shape: {cnn_in.shape}\")\n",
    "        cnn_out = self.conv1d(cnn_in)  # Shape: (batch_size, cnn_channels, seq_len)\n",
    "#         print(f\"CNN output shape before pooling: {cnn_out.shape}\")\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)\n",
    "#         print(f\"CNN output shape after pooling: {cnn_out.shape}\")\n",
    "\n",
    "        # Concatenate LSTM and CNN outputs along the feature dimension\n",
    "        combined_out = torch.cat((lstm_out, cnn_out), dim=1)\n",
    "#         print(f\"Combined output shape: {combined_out.shape}\")\n",
    "        output = self.fc(combined_out)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "input_size = features_scaled.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "cnn_channels = 32\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(input_size, hidden_size, num_layers, cnn_channels).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "# on all 84.91848756703507\n",
    "# on 14 33.31\n",
    "# hidden=64====> on all 76.67486999544822 for train i used only unit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63834a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09b74d88",
   "metadata": {},
   "source": [
    "# BiLSTM with Hidden size=32 and CNN=32 and Attention(XLSTM) Loss=74,97037406072378 i trained only using unit 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5d280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4edd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming df_unit1 is already defined\n",
    "features = train.drop(columns=['RUL', \"unit\", \"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension if needed\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class XLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels, attn_size):\n",
    "        super(XLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.attention = nn.Linear(hidden_size * 2, attn_size)\n",
    "        self.attn_combine = nn.Linear(attn_size, 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size * 2 + cnn_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        print(f\"LSTM output shape: {lstm_out.shape}\")\n",
    "        lstm_out = lstm_out[:, -1, :]  # Use the output from the last time step\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        print(f\"LSTM last time step output shape: {lstm_out.shape}\")\n",
    "\n",
    "        attn_weights = torch.tanh(self.attention(lstm_out))\n",
    "        attn_weights = self.attn_combine(attn_weights)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_output = torch.bmm(attn_weights.unsqueeze(1), lstm_out.unsqueeze(1))\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        print(f\"Attention output shape: {attn_output.shape}\")\n",
    "\n",
    "        cnn_in = x.squeeze(1).unsqueeze(1)\n",
    "        print(f\"CNN input shape: {cnn_in.shape}\")\n",
    "        cnn_out = self.conv1d(cnn_in)\n",
    "        print(f\"CNN output shape before pooling: {cnn_out.shape}\")\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)\n",
    "        print(f\"CNN output shape after pooling: {cnn_out.shape}\")\n",
    "\n",
    "        combined_out = torch.cat((attn_output, cnn_out), dim=1)\n",
    "        print(f\"Combined output shape: {combined_out.shape}\")\n",
    "        output = self.fc(combined_out)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "input_size = features_scaled.shape[1]\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "cnn_channels = 32\n",
    "attn_size = 64\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = XLSTM(input_size, hidden_size, num_layers, cnn_channels, attn_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "# 74.97037406072378 on all test and was trained only using 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d02807",
   "metadata": {},
   "source": [
    "# BiLSTM Hidden=64 and CNN=32 and attention trained only on unit 2 loss= 75.89781064193672"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734edda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming df_unit1 is already defined\n",
    "features = train.drop(columns=['RUL', \"unit\", \"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension if needed\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class XLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels, attn_size):\n",
    "        super(XLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.attention = nn.Linear(hidden_size * 2, attn_size)\n",
    "        self.attn_combine = nn.Linear(attn_size, 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size * 2 + cnn_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        print(f\"LSTM output shape: {lstm_out.shape}\")\n",
    "        lstm_out = lstm_out[:, -1, :]  # Use the output from the last time step\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        print(f\"LSTM last time step output shape: {lstm_out.shape}\")\n",
    "\n",
    "        attn_weights = torch.tanh(self.attention(lstm_out))\n",
    "        attn_weights = self.attn_combine(attn_weights)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_output = torch.bmm(attn_weights.unsqueeze(1), lstm_out.unsqueeze(1))\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        print(f\"Attention output shape: {attn_output.shape}\")\n",
    "\n",
    "        cnn_in = x.squeeze(1).unsqueeze(1)\n",
    "        print(f\"CNN input shape: {cnn_in.shape}\")\n",
    "        cnn_out = self.conv1d(cnn_in)\n",
    "        print(f\"CNN output shape before pooling: {cnn_out.shape}\")\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)\n",
    "        print(f\"CNN output shape after pooling: {cnn_out.shape}\")\n",
    "\n",
    "        combined_out = torch.cat((attn_output, cnn_out), dim=1)\n",
    "        print(f\"Combined output shape: {combined_out.shape}\")\n",
    "        output = self.fc(combined_out)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "input_size = features_scaled.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "cnn_channels = 32\n",
    "attn_size = 64\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = XLSTM(input_size, hidden_size, num_layers, cnn_channels, attn_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d722671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_unit1 is already defined\n",
    "def evaluation(test):\n",
    "    features = test.drop(columns=['RUL',\"unit\",\"Fc\"]).values\n",
    "    target = test['RUL'].values.reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    dataset = CustomDataset(features_scaled, target)\n",
    "    test_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    i=0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in test_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            i=i+1\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    # Calculate average test loss\n",
    "    test_loss /= len(test_dataloader)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7db5379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor()\n\u001b[0;32m      5\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFE(model, n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# Select the top 10 features\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m fit \u001b[38;5;241m=\u001b[39m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m fit\u001b[38;5;241m.\u001b[39mtransform(features_scaled)\n\u001b[0;32m      9\u001b[0m selected_feature_names \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mcolumns[fit\u001b[38;5;241m.\u001b[39msupport_]\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:264\u001b[0m, in \u001b[0;36mRFE.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:311\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 311\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    314\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    315\u001b[0m     estimator,\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    317\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    318\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "rfe = RFE(model, n_features_to_select=10)  # Select the top 10 features\n",
    "fit = rfe.fit(features_scaled, target)\n",
    "\n",
    "selected_features = fit.transform(features_scaled)\n",
    "selected_feature_names = train.columns[fit.support_]\n",
    "selected_feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34d05a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fit \u001b[38;5;241m=\u001b[39m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m fit\u001b[38;5;241m.\u001b[39mtransform(features_scaled)\n\u001b[0;32m      3\u001b[0m selected_feature_names \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mcolumns[fit\u001b[38;5;241m.\u001b[39msupport_]\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:264\u001b[0m, in \u001b[0;36mRFE.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:311\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 311\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    314\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    315\u001b[0m     estimator,\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    317\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    318\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit = rfe.fit(features_scaled, target)\n",
    "selected_features = fit.transform(features_scaled)\n",
    "selected_feature_names = train.columns[fit.support_]\n",
    "selected_feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2b31393",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'selected_feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mselected_feature_names\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'selected_feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "selected_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b5f8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111.32087716861648"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1432046",
   "metadata": {},
   "source": [
    "# BiLSTM hidden=64 CNN=64 attention trained only on unit 2 loss=72.60894099338253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc57c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming df_unit1 is already defined\n",
    "features = train.drop(columns=['RUL', \"unit\", \"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension if needed\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class XLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels, attn_size):\n",
    "        super(XLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.attention = nn.Linear(hidden_size * 2, attn_size)\n",
    "        self.attn_combine = nn.Linear(attn_size, 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size * 2 + cnn_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        print(f\"LSTM output shape: {lstm_out.shape}\")\n",
    "        lstm_out = lstm_out[:, -1, :]  # Use the output from the last time step\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        print(f\"LSTM last time step output shape: {lstm_out.shape}\")\n",
    "\n",
    "        attn_weights = torch.tanh(self.attention(lstm_out))\n",
    "        attn_weights = self.attn_combine(attn_weights)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_output = torch.bmm(attn_weights.unsqueeze(1), lstm_out.unsqueeze(1))\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        print(f\"Attention output shape: {attn_output.shape}\")\n",
    "\n",
    "        cnn_in = x.squeeze(1).unsqueeze(1)\n",
    "        print(f\"CNN input shape: {cnn_in.shape}\")\n",
    "        cnn_out = self.conv1d(cnn_in)\n",
    "        print(f\"CNN output shape before pooling: {cnn_out.shape}\")\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)\n",
    "        print(f\"CNN output shape after pooling: {cnn_out.shape}\")\n",
    "\n",
    "        combined_out = torch.cat((attn_output, cnn_out), dim=1)\n",
    "        print(f\"Combined output shape: {combined_out.shape}\")\n",
    "        output = self.fc(combined_out)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "input_size = features_scaled.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "cnn_channels = 64\n",
    "attn_size = 64\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = XLSTM(input_size, hidden_size, num_layers, cnn_channels, attn_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1c1bf",
   "metadata": {},
   "source": [
    "# BiLSTM hidden=64 CNN=64 attention trained the entireity of DS002 loss=195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24ff90f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 25.6386\n",
      "Epoch [2/10], Loss: 12.7439\n",
      "Epoch [3/10], Loss: 40.8834\n",
      "Epoch [4/10], Loss: 11.0838\n",
      "Epoch [5/10], Loss: 11.0820\n",
      "Epoch [6/10], Loss: 4.9783\n",
      "Epoch [7/10], Loss: 14.3930\n",
      "Epoch [8/10], Loss: 4.9185\n",
      "Epoch [9/10], Loss: 10.0067\n",
      "Epoch [10/10], Loss: 6.6674\n",
      "Training time: 3119.24 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming df_unit1 is already defined\n",
    "features = train.drop(columns=['RUL', \"unit\", \"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension if needed\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class XLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels, attn_size):\n",
    "        super(XLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.attention = nn.Linear(hidden_size * 2, attn_size)\n",
    "        self.attn_combine = nn.Linear(attn_size, 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size * 2 + cnn_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(f\"Input shape: {x.shape}\")\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "#         print(f\"LSTM output shape: {lstm_out.shape}\")\n",
    "        lstm_out = lstm_out[:, -1, :]  # Use the output from the last time step\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "#         print(f\"LSTM last time step output shape: {lstm_out.shape}\")\n",
    "\n",
    "        attn_weights = torch.tanh(self.attention(lstm_out))\n",
    "        attn_weights = self.attn_combine(attn_weights)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_output = torch.bmm(attn_weights.unsqueeze(1), lstm_out.unsqueeze(1))\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "#         print(f\"Attention output shape: {attn_output.shape}\")\n",
    "\n",
    "        cnn_in = x.squeeze(1).unsqueeze(1)\n",
    "#         print(f\"CNN input shape: {cnn_in.shape}\")\n",
    "        cnn_out = self.conv1d(cnn_in)\n",
    "#         print(f\"CNN output shape before pooling: {cnn_out.shape}\")\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)\n",
    "#         print(f\"CNN output shape after pooling: {cnn_out.shape}\")\n",
    "\n",
    "        combined_out = torch.cat((attn_output, cnn_out), dim=1)\n",
    "#         print(f\"Combined output shape: {combined_out.shape}\")\n",
    "        output = self.fc(combined_out)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "input_size = features_scaled.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "cnn_channels = 64\n",
    "attn_size = 64\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = XLSTM(input_size, hidden_size, num_layers, cnn_channels, attn_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ff164",
   "metadata": {},
   "source": [
    "# BiLSTM hidden=16 CNN=16 with attention trained on unit2 only loss=73.28647924435874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ec47973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 7.2250\n",
      "Epoch [2/10], Loss: 1.5078\n",
      "Epoch [3/10], Loss: 4.1231\n",
      "Epoch [4/10], Loss: 5.8868\n",
      "Epoch [5/10], Loss: 4.4517\n",
      "Epoch [6/10], Loss: 4.8269\n",
      "Epoch [7/10], Loss: 3.7699\n",
      "Epoch [8/10], Loss: 1.9653\n",
      "Epoch [9/10], Loss: 1.4357\n",
      "Epoch [10/10], Loss: 0.7280\n",
      "Training time: 620.33 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming df_unit1 is already defined\n",
    "features = train.drop(columns=['RUL', \"unit\", \"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension if needed\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class XLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels, attn_size):\n",
    "        super(XLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.attention = nn.Linear(hidden_size * 2, attn_size)\n",
    "        self.attn_combine = nn.Linear(attn_size, 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size * 2 + cnn_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(f\"Input shape: {x.shape}\")\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "#         print(f\"LSTM output shape: {lstm_out.shape}\")\n",
    "        lstm_out = lstm_out[:, -1, :]  # Use the output from the last time step\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "#         print(f\"LSTM last time step output shape: {lstm_out.shape}\")\n",
    "\n",
    "        attn_weights = torch.tanh(self.attention(lstm_out))\n",
    "        attn_weights = self.attn_combine(attn_weights)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_output = torch.bmm(attn_weights.unsqueeze(1), lstm_out.unsqueeze(1))\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "#         print(f\"Attention output shape: {attn_output.shape}\")\n",
    "\n",
    "        cnn_in = x.squeeze(1).unsqueeze(1)\n",
    "#         print(f\"CNN input shape: {cnn_in.shape}\")\n",
    "        cnn_out = self.conv1d(cnn_in)\n",
    "#         print(f\"CNN output shape before pooling: {cnn_out.shape}\")\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)\n",
    "#         print(f\"CNN output shape after pooling: {cnn_out.shape}\")\n",
    "\n",
    "        combined_out = torch.cat((attn_output, cnn_out), dim=1)\n",
    "#         print(f\"Combined output shape: {combined_out.shape}\")\n",
    "        output = self.fc(combined_out)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "input_size = features_scaled.shape[1]\n",
    "hidden_size = 16\n",
    "num_layers = 2\n",
    "cnn_channels = 16\n",
    "attn_size = 32\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = XLSTM(input_size, hidden_size, num_layers, cnn_channels, attn_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb739b82",
   "metadata": {},
   "source": [
    "# BiLSTM hidden=32 CNN=32 with attention  and a dropout rate=0.3 trained on unit2 only loss=47.05092177008901 and the second time i got 38.9295559518005 third time i got 34.09007194503465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0c97a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 9.8661\n",
      "Epoch [2/10], Loss: 5.0385\n",
      "Epoch [3/10], Loss: 3.7772\n",
      "Epoch [4/10], Loss: 5.7105\n",
      "Epoch [5/10], Loss: 6.7234\n",
      "Epoch [6/10], Loss: 0.7437\n",
      "Epoch [7/10], Loss: 1.3912\n",
      "Epoch [8/10], Loss: 1.3090\n",
      "Epoch [9/10], Loss: 0.9219\n",
      "Epoch [10/10], Loss: 1.2544\n",
      "Training time: 729.24 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming df_unit1 is already defined\n",
    "features = train.drop(columns=['RUL', \"unit\", \"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension if needed\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class XLSTM_V2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels, attn_size, dropout_rate=0.5):\n",
    "        super(XLSTM_V2, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.attention = nn.Linear(hidden_size * 2, attn_size)\n",
    "        self.attn_combine = nn.Linear(attn_size, 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2 + cnn_channels, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Use the output from the last time step\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        attn_weights = torch.tanh(self.attention(lstm_out))\n",
    "        attn_weights = self.attn_combine(attn_weights)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_output = torch.bmm(attn_weights.unsqueeze(1), lstm_out.unsqueeze(1))\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "\n",
    "        cnn_in = x.squeeze(1).unsqueeze(1)\n",
    "        cnn_out = self.conv1d(cnn_in)\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)\n",
    "\n",
    "        combined_out = torch.cat((attn_output, cnn_out), dim=1)\n",
    "        combined_out = self.dropout(combined_out)\n",
    "        combined_out = self.relu(self.fc1(combined_out))\n",
    "        output = self.fc2(combined_out)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "input_size = features_scaled.shape[1]\n",
    "hidden_size = 32  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "cnn_channels = 32  # Increased CNN channels\n",
    "attn_size = 64  # Increased attention size\n",
    "dropout_rate = 0.3  # Dropout rate for regularization\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = XLSTM_V2(input_size, hidden_size, num_layers, cnn_channels, attn_size, dropout_rate).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a018d8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 17.3681\n",
      "Epoch [2/15], Loss: 5.3047\n",
      "Epoch [3/15], Loss: 4.3611\n",
      "Epoch [4/15], Loss: 6.3605\n",
      "Epoch [5/15], Loss: 7.9493\n",
      "Epoch [6/15], Loss: 6.5318\n",
      "Epoch [7/15], Loss: 3.7735\n",
      "Epoch [8/15], Loss: 1.8394\n",
      "Epoch [9/15], Loss: 1.3458\n",
      "Epoch [10/15], Loss: 2.4808\n",
      "Epoch [11/15], Loss: 1.9612\n",
      "Epoch [12/15], Loss: 0.9142\n",
      "Epoch [13/15], Loss: 2.1208\n",
      "Epoch [14/15], Loss: 1.0382\n",
      "Epoch [15/15], Loss: 1.7011\n",
      "Training time: 1379.37 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "\n",
    "features = train.drop(columns=['RUL', \"unit\", \"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension if needed\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class XLSTM_V2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels, attn_size, dropout_rate=0.5):\n",
    "        super(XLSTM_V2, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.attention = nn.Linear(hidden_size * 2, attn_size)\n",
    "        self.attn_combine = nn.Linear(attn_size, 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2 + cnn_channels, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Use the output from the last time step\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        attn_weights = torch.tanh(self.attention(lstm_out))\n",
    "        attn_weights = self.attn_combine(attn_weights)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_output = torch.bmm(attn_weights.unsqueeze(1), lstm_out.unsqueeze(1))\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "\n",
    "        cnn_in = x.squeeze(1).unsqueeze(1)\n",
    "        cnn_out = self.conv1d(cnn_in)\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)\n",
    "\n",
    "        combined_out = torch.cat((attn_output, cnn_out), dim=1)\n",
    "        combined_out = self.dropout(combined_out)\n",
    "        combined_out = self.relu(self.fc1(combined_out))\n",
    "        output = self.fc2(combined_out)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "input_size = features_scaled.shape[1]\n",
    "hidden_size = 16  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "cnn_channels = 64  # Increased CNN channels\n",
    "attn_size = 64  # Increased attention size\n",
    "dropout_rate = 0.3  # Dropout rate for regularization\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 15\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = XLSTM_V2(input_size, hidden_size, num_layers, cnn_channels, attn_size, dropout_rate).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61af7f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit</th>\n",
       "      <th>Fc</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>P2</th>\n",
       "      <th>P21</th>\n",
       "      <th>P24</th>\n",
       "      <th>...</th>\n",
       "      <th>W31</th>\n",
       "      <th>W32</th>\n",
       "      <th>W48</th>\n",
       "      <th>W50</th>\n",
       "      <th>SmFan</th>\n",
       "      <th>SmLPC</th>\n",
       "      <th>SmHPC</th>\n",
       "      <th>phi</th>\n",
       "      <th>HPT_eff_mod</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5263447</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.369822</td>\n",
       "      <td>1441.086963</td>\n",
       "      <td>1822.407728</td>\n",
       "      <td>1230.069061</td>\n",
       "      <td>15.900837</td>\n",
       "      <td>11.636834</td>\n",
       "      <td>16.142982</td>\n",
       "      <td>20.267629</td>\n",
       "      <td>...</td>\n",
       "      <td>26.649529</td>\n",
       "      <td>15.989717</td>\n",
       "      <td>217.085529</td>\n",
       "      <td>229.722454</td>\n",
       "      <td>16.745510</td>\n",
       "      <td>9.812495</td>\n",
       "      <td>25.345244</td>\n",
       "      <td>41.971419</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263448</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.381211</td>\n",
       "      <td>1441.055436</td>\n",
       "      <td>1822.376094</td>\n",
       "      <td>1230.025551</td>\n",
       "      <td>15.900690</td>\n",
       "      <td>11.637199</td>\n",
       "      <td>16.142833</td>\n",
       "      <td>20.267529</td>\n",
       "      <td>...</td>\n",
       "      <td>26.646358</td>\n",
       "      <td>15.987815</td>\n",
       "      <td>217.058720</td>\n",
       "      <td>229.694212</td>\n",
       "      <td>16.751997</td>\n",
       "      <td>9.806257</td>\n",
       "      <td>25.346932</td>\n",
       "      <td>41.971470</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263449</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.392126</td>\n",
       "      <td>1441.063188</td>\n",
       "      <td>1822.350721</td>\n",
       "      <td>1229.965758</td>\n",
       "      <td>15.899810</td>\n",
       "      <td>11.636655</td>\n",
       "      <td>16.141939</td>\n",
       "      <td>20.266716</td>\n",
       "      <td>...</td>\n",
       "      <td>26.644369</td>\n",
       "      <td>15.986621</td>\n",
       "      <td>217.043190</td>\n",
       "      <td>229.677650</td>\n",
       "      <td>16.758975</td>\n",
       "      <td>9.804009</td>\n",
       "      <td>25.348326</td>\n",
       "      <td>41.969940</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263450</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.348485</td>\n",
       "      <td>1440.964145</td>\n",
       "      <td>1822.141800</td>\n",
       "      <td>1229.809741</td>\n",
       "      <td>15.894349</td>\n",
       "      <td>11.632321</td>\n",
       "      <td>16.136395</td>\n",
       "      <td>20.260029</td>\n",
       "      <td>...</td>\n",
       "      <td>26.636854</td>\n",
       "      <td>15.982113</td>\n",
       "      <td>216.981145</td>\n",
       "      <td>229.612403</td>\n",
       "      <td>16.755378</td>\n",
       "      <td>9.803649</td>\n",
       "      <td>25.352080</td>\n",
       "      <td>41.964794</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263451</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.285695</td>\n",
       "      <td>1440.852510</td>\n",
       "      <td>1822.019760</td>\n",
       "      <td>1229.732630</td>\n",
       "      <td>15.886351</td>\n",
       "      <td>11.626363</td>\n",
       "      <td>16.128275</td>\n",
       "      <td>20.249683</td>\n",
       "      <td>...</td>\n",
       "      <td>26.625692</td>\n",
       "      <td>15.975415</td>\n",
       "      <td>216.890123</td>\n",
       "      <td>229.516137</td>\n",
       "      <td>16.753262</td>\n",
       "      <td>9.806697</td>\n",
       "      <td>25.351024</td>\n",
       "      <td>41.963540</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517185</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.098139</td>\n",
       "      <td>1203.357895</td>\n",
       "      <td>1421.337905</td>\n",
       "      <td>1077.482738</td>\n",
       "      <td>12.837452</td>\n",
       "      <td>11.143805</td>\n",
       "      <td>13.032946</td>\n",
       "      <td>14.657219</td>\n",
       "      <td>...</td>\n",
       "      <td>15.786529</td>\n",
       "      <td>9.471918</td>\n",
       "      <td>127.892362</td>\n",
       "      <td>135.472311</td>\n",
       "      <td>16.461637</td>\n",
       "      <td>6.788187</td>\n",
       "      <td>32.223755</td>\n",
       "      <td>33.147993</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517186</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.035020</td>\n",
       "      <td>1203.472392</td>\n",
       "      <td>1422.772517</td>\n",
       "      <td>1078.697718</td>\n",
       "      <td>12.835965</td>\n",
       "      <td>11.144114</td>\n",
       "      <td>13.031436</td>\n",
       "      <td>14.652980</td>\n",
       "      <td>...</td>\n",
       "      <td>15.786545</td>\n",
       "      <td>9.471927</td>\n",
       "      <td>127.892061</td>\n",
       "      <td>135.471205</td>\n",
       "      <td>16.451330</td>\n",
       "      <td>6.803624</td>\n",
       "      <td>32.156314</td>\n",
       "      <td>33.208375</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517187</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.055787</td>\n",
       "      <td>1203.507712</td>\n",
       "      <td>1422.445115</td>\n",
       "      <td>1078.410015</td>\n",
       "      <td>12.838463</td>\n",
       "      <td>11.145616</td>\n",
       "      <td>13.033973</td>\n",
       "      <td>14.656059</td>\n",
       "      <td>...</td>\n",
       "      <td>15.791111</td>\n",
       "      <td>9.474666</td>\n",
       "      <td>127.930662</td>\n",
       "      <td>135.511892</td>\n",
       "      <td>16.444379</td>\n",
       "      <td>6.810375</td>\n",
       "      <td>32.172889</td>\n",
       "      <td>33.191835</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517188</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.060142</td>\n",
       "      <td>1203.529870</td>\n",
       "      <td>1422.576000</td>\n",
       "      <td>1078.539340</td>\n",
       "      <td>12.839843</td>\n",
       "      <td>11.146680</td>\n",
       "      <td>13.035374</td>\n",
       "      <td>14.657504</td>\n",
       "      <td>...</td>\n",
       "      <td>15.792761</td>\n",
       "      <td>9.475657</td>\n",
       "      <td>127.943031</td>\n",
       "      <td>135.524933</td>\n",
       "      <td>16.435526</td>\n",
       "      <td>6.810105</td>\n",
       "      <td>32.168495</td>\n",
       "      <td>33.197271</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517189</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.066645</td>\n",
       "      <td>1203.566865</td>\n",
       "      <td>1422.622803</td>\n",
       "      <td>1078.589932</td>\n",
       "      <td>12.841603</td>\n",
       "      <td>11.147872</td>\n",
       "      <td>13.037160</td>\n",
       "      <td>14.659412</td>\n",
       "      <td>...</td>\n",
       "      <td>15.795688</td>\n",
       "      <td>9.477413</td>\n",
       "      <td>127.967373</td>\n",
       "      <td>135.550693</td>\n",
       "      <td>16.424305</td>\n",
       "      <td>6.812680</td>\n",
       "      <td>32.167040</td>\n",
       "      <td>33.197768</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1253743 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         unit   Fc         T24          T30          T48          T50  \\\n",
       "5263447  11.0  3.0  601.369822  1441.086963  1822.407728  1230.069061   \n",
       "5263448  11.0  3.0  601.381211  1441.055436  1822.376094  1230.025551   \n",
       "5263449  11.0  3.0  601.392126  1441.063188  1822.350721  1229.965758   \n",
       "5263450  11.0  3.0  601.348485  1440.964145  1822.141800  1229.809741   \n",
       "5263451  11.0  3.0  601.285695  1440.852510  1822.019760  1229.732630   \n",
       "...       ...  ...         ...          ...          ...          ...   \n",
       "6517185  15.0  2.0  544.098139  1203.357895  1421.337905  1077.482738   \n",
       "6517186  15.0  2.0  544.035020  1203.472392  1422.772517  1078.697718   \n",
       "6517187  15.0  2.0  544.055787  1203.507712  1422.445115  1078.410015   \n",
       "6517188  15.0  2.0  544.060142  1203.529870  1422.576000  1078.539340   \n",
       "6517189  15.0  2.0  544.066645  1203.566865  1422.622803  1078.589932   \n",
       "\n",
       "               P15         P2        P21        P24  ...        W31  \\\n",
       "5263447  15.900837  11.636834  16.142982  20.267629  ...  26.649529   \n",
       "5263448  15.900690  11.637199  16.142833  20.267529  ...  26.646358   \n",
       "5263449  15.899810  11.636655  16.141939  20.266716  ...  26.644369   \n",
       "5263450  15.894349  11.632321  16.136395  20.260029  ...  26.636854   \n",
       "5263451  15.886351  11.626363  16.128275  20.249683  ...  26.625692   \n",
       "...            ...        ...        ...        ...  ...        ...   \n",
       "6517185  12.837452  11.143805  13.032946  14.657219  ...  15.786529   \n",
       "6517186  12.835965  11.144114  13.031436  14.652980  ...  15.786545   \n",
       "6517187  12.838463  11.145616  13.033973  14.656059  ...  15.791111   \n",
       "6517188  12.839843  11.146680  13.035374  14.657504  ...  15.792761   \n",
       "6517189  12.841603  11.147872  13.037160  14.659412  ...  15.795688   \n",
       "\n",
       "               W32         W48         W50      SmFan     SmLPC      SmHPC  \\\n",
       "5263447  15.989717  217.085529  229.722454  16.745510  9.812495  25.345244   \n",
       "5263448  15.987815  217.058720  229.694212  16.751997  9.806257  25.346932   \n",
       "5263449  15.986621  217.043190  229.677650  16.758975  9.804009  25.348326   \n",
       "5263450  15.982113  216.981145  229.612403  16.755378  9.803649  25.352080   \n",
       "5263451  15.975415  216.890123  229.516137  16.753262  9.806697  25.351024   \n",
       "...            ...         ...         ...        ...       ...        ...   \n",
       "6517185   9.471918  127.892362  135.472311  16.461637  6.788187  32.223755   \n",
       "6517186   9.471927  127.892061  135.471205  16.451330  6.803624  32.156314   \n",
       "6517187   9.474666  127.930662  135.511892  16.444379  6.810375  32.172889   \n",
       "6517188   9.475657  127.943031  135.524933  16.435526  6.810105  32.168495   \n",
       "6517189   9.477413  127.967373  135.550693  16.424305  6.812680  32.167040   \n",
       "\n",
       "               phi  HPT_eff_mod  RUL  \n",
       "5263447  41.971419    -0.000698   58  \n",
       "5263448  41.971470    -0.000698   58  \n",
       "5263449  41.969940    -0.000698   58  \n",
       "5263450  41.964794    -0.000698   58  \n",
       "5263451  41.963540    -0.000698   58  \n",
       "...            ...          ...  ...  \n",
       "6517185  33.147993    -0.010829    0  \n",
       "6517186  33.208375    -0.010829    0  \n",
       "6517187  33.191835    -0.010829    0  \n",
       "6517188  33.197271    -0.010829    0  \n",
       "6517189  33.197768    -0.010829    0  \n",
       "\n",
       "[1253743 rows x 32 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aa11e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_unit1 is already defined\n",
    "def evaluation(test):\n",
    "    features = test.drop(columns=['RUL',\"unit\",\"Fc\"]).values\n",
    "    target = test['RUL'].values.reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    dataset = CustomDataset(features_scaled, target)\n",
    "    test_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, targets in test_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    # Calculate average test loss\n",
    "    test_loss /= len(test_dataloader)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a379b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit</th>\n",
       "      <th>Fc</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>P2</th>\n",
       "      <th>P21</th>\n",
       "      <th>P24</th>\n",
       "      <th>...</th>\n",
       "      <th>W31</th>\n",
       "      <th>W32</th>\n",
       "      <th>W48</th>\n",
       "      <th>W50</th>\n",
       "      <th>SmFan</th>\n",
       "      <th>SmLPC</th>\n",
       "      <th>SmHPC</th>\n",
       "      <th>phi</th>\n",
       "      <th>HPT_eff_mod</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5263447</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.369822</td>\n",
       "      <td>1441.086963</td>\n",
       "      <td>1822.407728</td>\n",
       "      <td>1230.069061</td>\n",
       "      <td>15.900837</td>\n",
       "      <td>11.636834</td>\n",
       "      <td>16.142982</td>\n",
       "      <td>20.267629</td>\n",
       "      <td>...</td>\n",
       "      <td>26.649529</td>\n",
       "      <td>15.989717</td>\n",
       "      <td>217.085529</td>\n",
       "      <td>229.722454</td>\n",
       "      <td>16.745510</td>\n",
       "      <td>9.812495</td>\n",
       "      <td>25.345244</td>\n",
       "      <td>41.971419</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263448</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.381211</td>\n",
       "      <td>1441.055436</td>\n",
       "      <td>1822.376094</td>\n",
       "      <td>1230.025551</td>\n",
       "      <td>15.900690</td>\n",
       "      <td>11.637199</td>\n",
       "      <td>16.142833</td>\n",
       "      <td>20.267529</td>\n",
       "      <td>...</td>\n",
       "      <td>26.646358</td>\n",
       "      <td>15.987815</td>\n",
       "      <td>217.058720</td>\n",
       "      <td>229.694212</td>\n",
       "      <td>16.751997</td>\n",
       "      <td>9.806257</td>\n",
       "      <td>25.346932</td>\n",
       "      <td>41.971470</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263449</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.392126</td>\n",
       "      <td>1441.063188</td>\n",
       "      <td>1822.350721</td>\n",
       "      <td>1229.965758</td>\n",
       "      <td>15.899810</td>\n",
       "      <td>11.636655</td>\n",
       "      <td>16.141939</td>\n",
       "      <td>20.266716</td>\n",
       "      <td>...</td>\n",
       "      <td>26.644369</td>\n",
       "      <td>15.986621</td>\n",
       "      <td>217.043190</td>\n",
       "      <td>229.677650</td>\n",
       "      <td>16.758975</td>\n",
       "      <td>9.804009</td>\n",
       "      <td>25.348326</td>\n",
       "      <td>41.969940</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263450</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.348485</td>\n",
       "      <td>1440.964145</td>\n",
       "      <td>1822.141800</td>\n",
       "      <td>1229.809741</td>\n",
       "      <td>15.894349</td>\n",
       "      <td>11.632321</td>\n",
       "      <td>16.136395</td>\n",
       "      <td>20.260029</td>\n",
       "      <td>...</td>\n",
       "      <td>26.636854</td>\n",
       "      <td>15.982113</td>\n",
       "      <td>216.981145</td>\n",
       "      <td>229.612403</td>\n",
       "      <td>16.755378</td>\n",
       "      <td>9.803649</td>\n",
       "      <td>25.352080</td>\n",
       "      <td>41.964794</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263451</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.285695</td>\n",
       "      <td>1440.852510</td>\n",
       "      <td>1822.019760</td>\n",
       "      <td>1229.732630</td>\n",
       "      <td>15.886351</td>\n",
       "      <td>11.626363</td>\n",
       "      <td>16.128275</td>\n",
       "      <td>20.249683</td>\n",
       "      <td>...</td>\n",
       "      <td>26.625692</td>\n",
       "      <td>15.975415</td>\n",
       "      <td>216.890123</td>\n",
       "      <td>229.516137</td>\n",
       "      <td>16.753262</td>\n",
       "      <td>9.806697</td>\n",
       "      <td>25.351024</td>\n",
       "      <td>41.963540</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517185</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.098139</td>\n",
       "      <td>1203.357895</td>\n",
       "      <td>1421.337905</td>\n",
       "      <td>1077.482738</td>\n",
       "      <td>12.837452</td>\n",
       "      <td>11.143805</td>\n",
       "      <td>13.032946</td>\n",
       "      <td>14.657219</td>\n",
       "      <td>...</td>\n",
       "      <td>15.786529</td>\n",
       "      <td>9.471918</td>\n",
       "      <td>127.892362</td>\n",
       "      <td>135.472311</td>\n",
       "      <td>16.461637</td>\n",
       "      <td>6.788187</td>\n",
       "      <td>32.223755</td>\n",
       "      <td>33.147993</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517186</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.035020</td>\n",
       "      <td>1203.472392</td>\n",
       "      <td>1422.772517</td>\n",
       "      <td>1078.697718</td>\n",
       "      <td>12.835965</td>\n",
       "      <td>11.144114</td>\n",
       "      <td>13.031436</td>\n",
       "      <td>14.652980</td>\n",
       "      <td>...</td>\n",
       "      <td>15.786545</td>\n",
       "      <td>9.471927</td>\n",
       "      <td>127.892061</td>\n",
       "      <td>135.471205</td>\n",
       "      <td>16.451330</td>\n",
       "      <td>6.803624</td>\n",
       "      <td>32.156314</td>\n",
       "      <td>33.208375</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517187</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.055787</td>\n",
       "      <td>1203.507712</td>\n",
       "      <td>1422.445115</td>\n",
       "      <td>1078.410015</td>\n",
       "      <td>12.838463</td>\n",
       "      <td>11.145616</td>\n",
       "      <td>13.033973</td>\n",
       "      <td>14.656059</td>\n",
       "      <td>...</td>\n",
       "      <td>15.791111</td>\n",
       "      <td>9.474666</td>\n",
       "      <td>127.930662</td>\n",
       "      <td>135.511892</td>\n",
       "      <td>16.444379</td>\n",
       "      <td>6.810375</td>\n",
       "      <td>32.172889</td>\n",
       "      <td>33.191835</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517188</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.060142</td>\n",
       "      <td>1203.529870</td>\n",
       "      <td>1422.576000</td>\n",
       "      <td>1078.539340</td>\n",
       "      <td>12.839843</td>\n",
       "      <td>11.146680</td>\n",
       "      <td>13.035374</td>\n",
       "      <td>14.657504</td>\n",
       "      <td>...</td>\n",
       "      <td>15.792761</td>\n",
       "      <td>9.475657</td>\n",
       "      <td>127.943031</td>\n",
       "      <td>135.524933</td>\n",
       "      <td>16.435526</td>\n",
       "      <td>6.810105</td>\n",
       "      <td>32.168495</td>\n",
       "      <td>33.197271</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517189</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.066645</td>\n",
       "      <td>1203.566865</td>\n",
       "      <td>1422.622803</td>\n",
       "      <td>1078.589932</td>\n",
       "      <td>12.841603</td>\n",
       "      <td>11.147872</td>\n",
       "      <td>13.037160</td>\n",
       "      <td>14.659412</td>\n",
       "      <td>...</td>\n",
       "      <td>15.795688</td>\n",
       "      <td>9.477413</td>\n",
       "      <td>127.967373</td>\n",
       "      <td>135.550693</td>\n",
       "      <td>16.424305</td>\n",
       "      <td>6.812680</td>\n",
       "      <td>32.167040</td>\n",
       "      <td>33.197768</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1253743 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         unit   Fc         T24          T30          T48          T50  \\\n",
       "5263447  11.0  3.0  601.369822  1441.086963  1822.407728  1230.069061   \n",
       "5263448  11.0  3.0  601.381211  1441.055436  1822.376094  1230.025551   \n",
       "5263449  11.0  3.0  601.392126  1441.063188  1822.350721  1229.965758   \n",
       "5263450  11.0  3.0  601.348485  1440.964145  1822.141800  1229.809741   \n",
       "5263451  11.0  3.0  601.285695  1440.852510  1822.019760  1229.732630   \n",
       "...       ...  ...         ...          ...          ...          ...   \n",
       "6517185  15.0  2.0  544.098139  1203.357895  1421.337905  1077.482738   \n",
       "6517186  15.0  2.0  544.035020  1203.472392  1422.772517  1078.697718   \n",
       "6517187  15.0  2.0  544.055787  1203.507712  1422.445115  1078.410015   \n",
       "6517188  15.0  2.0  544.060142  1203.529870  1422.576000  1078.539340   \n",
       "6517189  15.0  2.0  544.066645  1203.566865  1422.622803  1078.589932   \n",
       "\n",
       "               P15         P2        P21        P24  ...        W31  \\\n",
       "5263447  15.900837  11.636834  16.142982  20.267629  ...  26.649529   \n",
       "5263448  15.900690  11.637199  16.142833  20.267529  ...  26.646358   \n",
       "5263449  15.899810  11.636655  16.141939  20.266716  ...  26.644369   \n",
       "5263450  15.894349  11.632321  16.136395  20.260029  ...  26.636854   \n",
       "5263451  15.886351  11.626363  16.128275  20.249683  ...  26.625692   \n",
       "...            ...        ...        ...        ...  ...        ...   \n",
       "6517185  12.837452  11.143805  13.032946  14.657219  ...  15.786529   \n",
       "6517186  12.835965  11.144114  13.031436  14.652980  ...  15.786545   \n",
       "6517187  12.838463  11.145616  13.033973  14.656059  ...  15.791111   \n",
       "6517188  12.839843  11.146680  13.035374  14.657504  ...  15.792761   \n",
       "6517189  12.841603  11.147872  13.037160  14.659412  ...  15.795688   \n",
       "\n",
       "               W32         W48         W50      SmFan     SmLPC      SmHPC  \\\n",
       "5263447  15.989717  217.085529  229.722454  16.745510  9.812495  25.345244   \n",
       "5263448  15.987815  217.058720  229.694212  16.751997  9.806257  25.346932   \n",
       "5263449  15.986621  217.043190  229.677650  16.758975  9.804009  25.348326   \n",
       "5263450  15.982113  216.981145  229.612403  16.755378  9.803649  25.352080   \n",
       "5263451  15.975415  216.890123  229.516137  16.753262  9.806697  25.351024   \n",
       "...            ...         ...         ...        ...       ...        ...   \n",
       "6517185   9.471918  127.892362  135.472311  16.461637  6.788187  32.223755   \n",
       "6517186   9.471927  127.892061  135.471205  16.451330  6.803624  32.156314   \n",
       "6517187   9.474666  127.930662  135.511892  16.444379  6.810375  32.172889   \n",
       "6517188   9.475657  127.943031  135.524933  16.435526  6.810105  32.168495   \n",
       "6517189   9.477413  127.967373  135.550693  16.424305  6.812680  32.167040   \n",
       "\n",
       "               phi  HPT_eff_mod  RUL  \n",
       "5263447  41.971419    -0.000698   58  \n",
       "5263448  41.971470    -0.000698   58  \n",
       "5263449  41.969940    -0.000698   58  \n",
       "5263450  41.964794    -0.000698   58  \n",
       "5263451  41.963540    -0.000698   58  \n",
       "...            ...          ...  ...  \n",
       "6517185  33.147993    -0.010829    0  \n",
       "6517186  33.208375    -0.010829    0  \n",
       "6517187  33.191835    -0.010829    0  \n",
       "6517188  33.197271    -0.010829    0  \n",
       "6517189  33.197768    -0.010829    0  \n",
       "\n",
       "[1253743 rows x 32 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dbc188b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.0870618137673"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(test)\n",
    "# 79.8403990427653\n",
    "# 80.28963890572237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "895e1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have a model instance 'model'\n",
    "# Save the model state_dict\n",
    "torch.save(model.state_dict(), 'model_cnn_bi_better_rmse.pth')\n",
    "\n",
    "# To load the model state_dict\n",
    "# model.load_state_dict(torch.load('model_cnn_bi.pth'))\n",
    "# model.eval()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eeb4f7",
   "metadata": {},
   "source": [
    "# residual connections, layer normalization, and deeper attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "601ec693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 6.3233\n",
      "Epoch [2/10], Loss: 2.5464\n",
      "Epoch [3/10], Loss: 1.9659\n",
      "Epoch [4/10], Loss: 1.8809\n",
      "Epoch [5/10], Loss: 1.3529\n",
      "Epoch [6/10], Loss: 0.8345\n",
      "Epoch [7/10], Loss: 0.6030\n",
      "Epoch [8/10], Loss: 0.3402\n",
      "Epoch [9/10], Loss: 0.3885\n",
      "Epoch [10/10], Loss: 0.5069\n",
      "Training time: 719.12 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming train is already defined and loaded with your training data\n",
    "features = train.drop(columns=['RUL', \"unit\", \"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension if needed\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class XLSTM_V2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels, attn_size, dropout_rate=0.5):\n",
    "        super(XLSTM_V2, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.attention = nn.Linear(hidden_size * 2, attn_size)\n",
    "        self.attn_combine = nn.Linear(attn_size, 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2 + cnn_channels, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.tanh(self.attention(lstm_out))\n",
    "        attn_weights = self.attn_combine(attn_weights).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        attn_output = torch.bmm(attn_weights, lstm_out).squeeze(1)\n",
    "\n",
    "        # CNN\n",
    "        cnn_in = x.squeeze(1).unsqueeze(1)\n",
    "        cnn_out = self.conv1d(cnn_in)\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)\n",
    "\n",
    "        # Combine LSTM and CNN outputs\n",
    "        combined_out = torch.cat((attn_output, cnn_out), dim=1)\n",
    "        combined_out = self.dropout(combined_out)\n",
    "        combined_out = self.relu(self.fc1(combined_out))\n",
    "        output = self.fc2(combined_out)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "input_size = features_scaled.shape[1]\n",
    "hidden_size = 32  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "cnn_channels = 32  # Increased CNN channels\n",
    "attn_size = 64  # Increased attention size\n",
    "dropout_rate = 0.3  # Dropout rate for regularization\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = XLSTM_V2(input_size, hidden_size, num_layers, cnn_channels, attn_size, dropout_rate).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbfdc81",
   "metadata": {},
   "source": [
    "#  added tranformer loss=84.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96640d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.3174\n",
      "Epoch [2/10], Loss: 4.1060\n",
      "Epoch [3/10], Loss: 1.9956\n",
      "Epoch [4/10], Loss: 1.5094\n",
      "Epoch [5/10], Loss: 2.8562\n",
      "Epoch [6/10], Loss: 1.2577\n",
      "Epoch [7/10], Loss: 0.8690\n",
      "Epoch [8/10], Loss: 1.8958\n",
      "Epoch [9/10], Loss: 0.4142\n",
      "Epoch [10/10], Loss: 0.7861\n",
      "Training time: 982.49 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming train is already defined and loaded with your training data\n",
    "features = train.drop(columns=['RUL', \"unit\", \"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension if needed\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query):\n",
    "        attention = self.attention(query, key, value)[0]\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class XLSTM_V3(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels, attn_size, dropout_rate=0.5):\n",
    "        super(XLSTM_V3, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size * 2)\n",
    "        self.transformer = TransformerBlock(embed_size=hidden_size * 2, heads=4, dropout=dropout_rate, forward_expansion=4)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size * 2)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2 + cnn_channels, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.layer_norm1(lstm_out)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        transformer_out = self.transformer(lstm_out, lstm_out, lstm_out)\n",
    "        transformer_out = self.layer_norm2(transformer_out)\n",
    "\n",
    "        # CNN\n",
    "        cnn_in = x.squeeze(1).unsqueeze(1)\n",
    "        cnn_out = self.conv1d(cnn_in)\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)\n",
    "\n",
    "        # Combine LSTM+Transformer and CNN outputs\n",
    "        combined_out = torch.cat((transformer_out[:, -1, :], cnn_out), dim=1)\n",
    "        combined_out = self.dropout(combined_out)\n",
    "        combined_out = self.relu(self.fc1(combined_out))\n",
    "        output = self.fc2(combined_out)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "input_size = features_scaled.shape[1]\n",
    "hidden_size = 32  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "cnn_channels = 32  # Increased CNN channels\n",
    "attn_size = 64  # Increased attention size\n",
    "dropout_rate = 0.3  # Dropout rate for regularization\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = XLSTM_V3(input_size, hidden_size, num_layers, cnn_channels, attn_size, dropout_rate).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949df3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.95948318765747"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5f287618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1253743"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e91d62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unit', 'Fc', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24',\n",
       "       'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf', 'T40', 'P30', 'P45', 'W21',\n",
       "       'W22', 'W25', 'W31', 'W32', 'W48', 'W50', 'SmFan', 'SmLPC', 'SmHPC',\n",
       "       'phi', 'LPC_flow_mod', 'HPT_eff_mod', 'RUL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78f3d8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit</th>\n",
       "      <th>Fc</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>P2</th>\n",
       "      <th>P21</th>\n",
       "      <th>P24</th>\n",
       "      <th>...</th>\n",
       "      <th>W32</th>\n",
       "      <th>W48</th>\n",
       "      <th>W50</th>\n",
       "      <th>SmFan</th>\n",
       "      <th>SmLPC</th>\n",
       "      <th>SmHPC</th>\n",
       "      <th>phi</th>\n",
       "      <th>LPC_flow_mod</th>\n",
       "      <th>HPT_eff_mod</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5263447</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.369822</td>\n",
       "      <td>1441.086963</td>\n",
       "      <td>1822.407728</td>\n",
       "      <td>1230.069061</td>\n",
       "      <td>15.900837</td>\n",
       "      <td>11.636834</td>\n",
       "      <td>16.142982</td>\n",
       "      <td>20.267629</td>\n",
       "      <td>...</td>\n",
       "      <td>15.989717</td>\n",
       "      <td>217.085529</td>\n",
       "      <td>229.722454</td>\n",
       "      <td>16.745510</td>\n",
       "      <td>9.812495</td>\n",
       "      <td>25.345244</td>\n",
       "      <td>41.971419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263448</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.381211</td>\n",
       "      <td>1441.055436</td>\n",
       "      <td>1822.376094</td>\n",
       "      <td>1230.025551</td>\n",
       "      <td>15.900690</td>\n",
       "      <td>11.637199</td>\n",
       "      <td>16.142833</td>\n",
       "      <td>20.267529</td>\n",
       "      <td>...</td>\n",
       "      <td>15.987815</td>\n",
       "      <td>217.058720</td>\n",
       "      <td>229.694212</td>\n",
       "      <td>16.751997</td>\n",
       "      <td>9.806257</td>\n",
       "      <td>25.346932</td>\n",
       "      <td>41.971470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263449</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.392126</td>\n",
       "      <td>1441.063188</td>\n",
       "      <td>1822.350721</td>\n",
       "      <td>1229.965758</td>\n",
       "      <td>15.899810</td>\n",
       "      <td>11.636655</td>\n",
       "      <td>16.141939</td>\n",
       "      <td>20.266716</td>\n",
       "      <td>...</td>\n",
       "      <td>15.986621</td>\n",
       "      <td>217.043190</td>\n",
       "      <td>229.677650</td>\n",
       "      <td>16.758975</td>\n",
       "      <td>9.804009</td>\n",
       "      <td>25.348326</td>\n",
       "      <td>41.969940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263450</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.348485</td>\n",
       "      <td>1440.964145</td>\n",
       "      <td>1822.141800</td>\n",
       "      <td>1229.809741</td>\n",
       "      <td>15.894349</td>\n",
       "      <td>11.632321</td>\n",
       "      <td>16.136395</td>\n",
       "      <td>20.260029</td>\n",
       "      <td>...</td>\n",
       "      <td>15.982113</td>\n",
       "      <td>216.981145</td>\n",
       "      <td>229.612403</td>\n",
       "      <td>16.755378</td>\n",
       "      <td>9.803649</td>\n",
       "      <td>25.352080</td>\n",
       "      <td>41.964794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263451</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>601.285695</td>\n",
       "      <td>1440.852510</td>\n",
       "      <td>1822.019760</td>\n",
       "      <td>1229.732630</td>\n",
       "      <td>15.886351</td>\n",
       "      <td>11.626363</td>\n",
       "      <td>16.128275</td>\n",
       "      <td>20.249683</td>\n",
       "      <td>...</td>\n",
       "      <td>15.975415</td>\n",
       "      <td>216.890123</td>\n",
       "      <td>229.516137</td>\n",
       "      <td>16.753262</td>\n",
       "      <td>9.806697</td>\n",
       "      <td>25.351024</td>\n",
       "      <td>41.963540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517185</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.098139</td>\n",
       "      <td>1203.357895</td>\n",
       "      <td>1421.337905</td>\n",
       "      <td>1077.482738</td>\n",
       "      <td>12.837452</td>\n",
       "      <td>11.143805</td>\n",
       "      <td>13.032946</td>\n",
       "      <td>14.657219</td>\n",
       "      <td>...</td>\n",
       "      <td>9.471918</td>\n",
       "      <td>127.892362</td>\n",
       "      <td>135.472311</td>\n",
       "      <td>16.461637</td>\n",
       "      <td>6.788187</td>\n",
       "      <td>32.223755</td>\n",
       "      <td>33.147993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517186</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.035020</td>\n",
       "      <td>1203.472392</td>\n",
       "      <td>1422.772517</td>\n",
       "      <td>1078.697718</td>\n",
       "      <td>12.835965</td>\n",
       "      <td>11.144114</td>\n",
       "      <td>13.031436</td>\n",
       "      <td>14.652980</td>\n",
       "      <td>...</td>\n",
       "      <td>9.471927</td>\n",
       "      <td>127.892061</td>\n",
       "      <td>135.471205</td>\n",
       "      <td>16.451330</td>\n",
       "      <td>6.803624</td>\n",
       "      <td>32.156314</td>\n",
       "      <td>33.208375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517187</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.055787</td>\n",
       "      <td>1203.507712</td>\n",
       "      <td>1422.445115</td>\n",
       "      <td>1078.410015</td>\n",
       "      <td>12.838463</td>\n",
       "      <td>11.145616</td>\n",
       "      <td>13.033973</td>\n",
       "      <td>14.656059</td>\n",
       "      <td>...</td>\n",
       "      <td>9.474666</td>\n",
       "      <td>127.930662</td>\n",
       "      <td>135.511892</td>\n",
       "      <td>16.444379</td>\n",
       "      <td>6.810375</td>\n",
       "      <td>32.172889</td>\n",
       "      <td>33.191835</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517188</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.060142</td>\n",
       "      <td>1203.529870</td>\n",
       "      <td>1422.576000</td>\n",
       "      <td>1078.539340</td>\n",
       "      <td>12.839843</td>\n",
       "      <td>11.146680</td>\n",
       "      <td>13.035374</td>\n",
       "      <td>14.657504</td>\n",
       "      <td>...</td>\n",
       "      <td>9.475657</td>\n",
       "      <td>127.943031</td>\n",
       "      <td>135.524933</td>\n",
       "      <td>16.435526</td>\n",
       "      <td>6.810105</td>\n",
       "      <td>32.168495</td>\n",
       "      <td>33.197271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517189</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>544.066645</td>\n",
       "      <td>1203.566865</td>\n",
       "      <td>1422.622803</td>\n",
       "      <td>1078.589932</td>\n",
       "      <td>12.841603</td>\n",
       "      <td>11.147872</td>\n",
       "      <td>13.037160</td>\n",
       "      <td>14.659412</td>\n",
       "      <td>...</td>\n",
       "      <td>9.477413</td>\n",
       "      <td>127.967373</td>\n",
       "      <td>135.550693</td>\n",
       "      <td>16.424305</td>\n",
       "      <td>6.812680</td>\n",
       "      <td>32.167040</td>\n",
       "      <td>33.197768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1253743 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         unit   Fc         T24          T30          T48          T50  \\\n",
       "5263447  11.0  3.0  601.369822  1441.086963  1822.407728  1230.069061   \n",
       "5263448  11.0  3.0  601.381211  1441.055436  1822.376094  1230.025551   \n",
       "5263449  11.0  3.0  601.392126  1441.063188  1822.350721  1229.965758   \n",
       "5263450  11.0  3.0  601.348485  1440.964145  1822.141800  1229.809741   \n",
       "5263451  11.0  3.0  601.285695  1440.852510  1822.019760  1229.732630   \n",
       "...       ...  ...         ...          ...          ...          ...   \n",
       "6517185  15.0  2.0  544.098139  1203.357895  1421.337905  1077.482738   \n",
       "6517186  15.0  2.0  544.035020  1203.472392  1422.772517  1078.697718   \n",
       "6517187  15.0  2.0  544.055787  1203.507712  1422.445115  1078.410015   \n",
       "6517188  15.0  2.0  544.060142  1203.529870  1422.576000  1078.539340   \n",
       "6517189  15.0  2.0  544.066645  1203.566865  1422.622803  1078.589932   \n",
       "\n",
       "               P15         P2        P21        P24  ...        W32  \\\n",
       "5263447  15.900837  11.636834  16.142982  20.267629  ...  15.989717   \n",
       "5263448  15.900690  11.637199  16.142833  20.267529  ...  15.987815   \n",
       "5263449  15.899810  11.636655  16.141939  20.266716  ...  15.986621   \n",
       "5263450  15.894349  11.632321  16.136395  20.260029  ...  15.982113   \n",
       "5263451  15.886351  11.626363  16.128275  20.249683  ...  15.975415   \n",
       "...            ...        ...        ...        ...  ...        ...   \n",
       "6517185  12.837452  11.143805  13.032946  14.657219  ...   9.471918   \n",
       "6517186  12.835965  11.144114  13.031436  14.652980  ...   9.471927   \n",
       "6517187  12.838463  11.145616  13.033973  14.656059  ...   9.474666   \n",
       "6517188  12.839843  11.146680  13.035374  14.657504  ...   9.475657   \n",
       "6517189  12.841603  11.147872  13.037160  14.659412  ...   9.477413   \n",
       "\n",
       "                W48         W50      SmFan     SmLPC      SmHPC        phi  \\\n",
       "5263447  217.085529  229.722454  16.745510  9.812495  25.345244  41.971419   \n",
       "5263448  217.058720  229.694212  16.751997  9.806257  25.346932  41.971470   \n",
       "5263449  217.043190  229.677650  16.758975  9.804009  25.348326  41.969940   \n",
       "5263450  216.981145  229.612403  16.755378  9.803649  25.352080  41.964794   \n",
       "5263451  216.890123  229.516137  16.753262  9.806697  25.351024  41.963540   \n",
       "...             ...         ...        ...       ...        ...        ...   \n",
       "6517185  127.892362  135.472311  16.461637  6.788187  32.223755  33.147993   \n",
       "6517186  127.892061  135.471205  16.451330  6.803624  32.156314  33.208375   \n",
       "6517187  127.930662  135.511892  16.444379  6.810375  32.172889  33.191835   \n",
       "6517188  127.943031  135.524933  16.435526  6.810105  32.168495  33.197271   \n",
       "6517189  127.967373  135.550693  16.424305  6.812680  32.167040  33.197768   \n",
       "\n",
       "         LPC_flow_mod  HPT_eff_mod  RUL  \n",
       "5263447           0.0    -0.000698   58  \n",
       "5263448           0.0    -0.000698   58  \n",
       "5263449           0.0    -0.000698   58  \n",
       "5263450           0.0    -0.000698   58  \n",
       "5263451           0.0    -0.000698   58  \n",
       "...               ...          ...  ...  \n",
       "6517185           0.0    -0.010829    0  \n",
       "6517186           0.0    -0.010829    0  \n",
       "6517187           0.0    -0.010829    0  \n",
       "6517188           0.0    -0.010829    0  \n",
       "6517189           0.0    -0.010829    0  \n",
       "\n",
       "[1253743 rows x 33 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fdd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "918a9ca6",
   "metadata": {},
   "source": [
    "# testing instance by instance to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ee90081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted RUL for row 1253742: 3.0048\n"
     ]
    }
   ],
   "source": [
    "# Select a specific row, e.g., row 10000\n",
    "row_index = 1253742\n",
    "row_data = test.iloc[row_index]\n",
    "\n",
    "# Extract features for the selected row\n",
    "row_features = row_data.drop(['RUL', 'unit', 'Fc']).values.reshape(1, -1)\n",
    "\n",
    "# Scale the features\n",
    "row_features_scaled = scaler.transform(row_features)\n",
    "\n",
    "# Convert to tensor\n",
    "row_features_tensor = torch.tensor(row_features_scaled, dtype=torch.float32).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "# Make predictions\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    prediction = model(row_features_tensor)\n",
    "\n",
    "# Move the prediction back to CPU and convert to numpy\n",
    "prediction = prediction.cpu().numpy()\n",
    "\n",
    "print(f'Predicted RUL for row {row_index}: {prediction[0][0]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec855a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "truth=[]\n",
    "for i in range(len(test)):\n",
    "    row_index = i\n",
    "    row_data = test.iloc[row_index]\n",
    "\n",
    "# Extract features for the selected row\n",
    "    row_features = row_data.drop(['RUL', 'unit', 'Fc']).values.reshape(1, -1)\n",
    "\n",
    "# Scale the features\n",
    "    row_features_scaled = scaler.transform(row_features)\n",
    "\n",
    "# Convert to tensor\n",
    "    row_features_tensor = torch.tensor(row_features_scaled, dtype=torch.float32).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "# Make predictions\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        prediction = model(row_features_tensor)\n",
    "\n",
    "# Move the prediction back to CPU and convert to numpy\n",
    "    prediction = prediction.cpu().numpy()\n",
    "    predictions.append(prediction)\n",
    "    truth.append(row_data[\"RUL\"])\n",
    "    \n",
    "\n",
    "    # print(f'Predicted RUL for row {row_index}: {prediction[0][0]:.4f}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51855d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 38.92970765621519\n"
     ]
    }
   ],
   "source": [
    "def calculate_mse(true_values, estimated_values):\n",
    "    # Check if the arrays have the same length\n",
    "    if len(true_values) != len(estimated_values):\n",
    "        raise ValueError(\"Arrays must have the same length\")\n",
    "\n",
    "    # Calculate the squared differences\n",
    "    squared_diff = [(true_val - est_val) ** 2 for true_val, est_val in zip(true_values, estimated_values)]\n",
    "\n",
    "    # Calculate the mean of squared differences\n",
    "    mse = sum(squared_diff) / len(true_values)\n",
    "\n",
    "    return mse\n",
    "\n",
    "# Example usage:\n",
    "true_values = [3, 5, 7, 9]\n",
    "estimated_values = [2, 4, 6, 8]\n",
    "\n",
    "mse = calculate_mse(truth,temp)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d5370df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming you have a model instance 'model'\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the model state_dict\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# torch.save(model.state_dict(), 'model.pth')\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# To load the model state_dict\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_cnn_bi.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have a model instance 'model'\n",
    "# Save the model state_dict\n",
    "# torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# To load the model state_dict\n",
    "model.load_state_dict(torch.load('model_cnn_bi.pth'))\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90879c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Assuming df_unit1 is already defined\n",
    "features = train.drop(columns=['RUL', \"unit\", \"Fc\"]).values\n",
    "target = train['RUL'].values.reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension if needed\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "class XLSTM_V2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, cnn_channels, attn_size, dropout_rate=0.5):\n",
    "        super(XLSTM_V2, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.attention = nn.Linear(hidden_size * 2, attn_size)\n",
    "        self.attn_combine = nn.Linear(attn_size, 1)\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=cnn_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2 + cnn_channels, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Use the output from the last time step\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        attn_weights = torch.tanh(self.attention(lstm_out))\n",
    "        attn_weights = self.attn_combine(attn_weights)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_output = torch.bmm(attn_weights.unsqueeze(1), lstm_out.unsqueeze(1))\n",
    "        attn_output = attn_output.squeeze(1)\n",
    "\n",
    "        cnn_in = x.squeeze(1).unsqueeze(1)\n",
    "        cnn_out = self.conv1d(cnn_in)\n",
    "        cnn_out = self.relu(cnn_out)\n",
    "        cnn_out = torch.max_pool1d(cnn_out, kernel_size=cnn_out.size(2)).squeeze(2)\n",
    "\n",
    "        combined_out = torch.cat((attn_output, cnn_out), dim=1)\n",
    "        combined_out = self.dropout(combined_out)\n",
    "        combined_out = self.relu(self.fc1(combined_out))\n",
    "        output = self.fc2(combined_out)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "input_size = features_scaled.shape[1]\n",
    "hidden_size = 32  # Increased hidden size\n",
    "num_layers = 3  # Increased number of layers\n",
    "cnn_channels = 32  # Increased CNN channels\n",
    "attn_size = 64  # Increased attention size\n",
    "dropout_rate = 0.3  # Dropout rate for regularization\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(features_scaled, target)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = XLSTM_V2(input_size, hidden_size, num_layers, cnn_channels, attn_size, dropout_rate).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14a16ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLSTM_V2(\n",
       "  (lstm): LSTM(30, 32, num_layers=3, batch_first=True, bidirectional=True)\n",
       "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (attention): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (attn_combine): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (conv1d): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc1): Linear(in_features=96, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming you have a model instance 'model'\n",
    "# Save the model state_dict\n",
    "# torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "# To load the model state_dict\n",
    "model.load_state_dict(torch.load('model_cnn_bi.pth'))\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6252f5b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'triu' from 'scipy.linalg' (c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\linalg\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnode2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Node2Vec\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a graph\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\node2vec\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m edges\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Node2Vec\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\node2vec\\edges.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEdgeEmbedder\u001b[39;00m(ABC):\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gensim\\matutils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_blas_funcs, triu\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m psi  \u001b[38;5;66;03m# gamma function utils\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'triu' from 'scipy.linalg' (c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\linalg\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "\n",
    "# Create a graph\n",
    "G = nx.fast_gnp_random_graph(n=100, p=0.5)\n",
    "\n",
    "# Initialize Node2Vec\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n",
    "\n",
    "# Fit model\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = {str(node): model.wv[str(node)] for node in G.nodes()}\n",
    "\n",
    "# Print the embedding for node 0\n",
    "print(embeddings['0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96bc9328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\lenovo legion\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement pytorch_geometric (from versions: none)\n",
      "ERROR: No matching distribution found for pytorch_geometric\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install --upgrade node2vec networkx gensim scipy\n",
    "%pip install pytorch_geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53086099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Normalize the dataframe if necessary\n",
    "df_normalized = (train - train.mean()) / train.std()\n",
    "\n",
    "# Convert dataframe to tensor\n",
    "data_tensor = torch.tensor(df_normalized.values, dtype=torch.float)\n",
    "\n",
    "# Number of time steps and features\n",
    "num_time_steps, num_features = data_tensor.shape\n",
    "\n",
    "# Create nodes: each (time step, feature) pair is a node\n",
    "# We can create an edge list based on temporal adjacency (each time step is connected to the next)\n",
    "edge_index = []\n",
    "\n",
    "# Connect each time step to the next for each feature\n",
    "for feature in range(num_features):\n",
    "    for time_step in range(num_time_steps - 1):\n",
    "        start = time_step * num_features + feature\n",
    "        end = (time_step + 1) * num_features + feature\n",
    "        edge_index.append([start, end])\n",
    "\n",
    "# Convert edge_index to tensor\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Feature matrix for the graph: each node's features are the corresponding values in the dataframe\n",
    "x = data_tensor.reshape(-1, 1)  # each node gets its corresponding value as feature\n",
    "\n",
    "# Assume RUL is provided in a separate array\n",
    "# Replace 'your_rul_values' with your actual RUL values, here we assume it is a list or an array\n",
    "your_rul_values = train[\"RUL\"]  # replace with your actual RUL values\n",
    "rul = torch.tensor(your_rul_values, dtype=torch.float)\n",
    "\n",
    "# Create a Data object\n",
    "data = Data(x=x, edge_index=edge_index, y=rul)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60ff23d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([853142])) that is different to the input size (torch.Size([28153686, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 96076367925648 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     27\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3366\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3363\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m   3365\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[1;32m-> 3366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 96076367925648 bytes."
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = GCN(in_channels=1, hidden_channels=16, out_channels=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
