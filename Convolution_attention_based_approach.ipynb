{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "\n",
    "# Set a specific random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2060'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "x = x.to(device)\n",
    "print(x.device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Operation time (min):  0.053125\n",
      "\n",
      "W shape: (6517190, 4)\n",
      "X_s shape: (6517190, 14)\n",
      "X_v shape: (6517190, 14)\n",
      "T shape: (6517190, 10)\n",
      "A shape: (6517190, 4)\n"
     ]
    }
   ],
   "source": [
    "# Time tracking, Operation time (min):  0.003\n",
    "filename=\"N-CMAPSS_DS02-006.h5\"\n",
    "t = time.process_time()  \n",
    "\n",
    "# Load data\n",
    "with h5py.File(filename, 'r') as hdf:\n",
    "        # Development set\n",
    "        W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "        X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "        X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "        A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        W_test = np.array(hdf.get('W_test'))           # W\n",
    "        X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "        X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "        T_test = np.array(hdf.get('T_test'))           # T\n",
    "        Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "        A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "        \n",
    "        # Varnams\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "        X_v_var = np.array(hdf.get('X_v_var')) \n",
    "        T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "        \n",
    "        # from np.array to list dtype U4/U5\n",
    "        W_var = list(np.array(W_var, dtype='U20'))\n",
    "        X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "        X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "        T_var = list(np.array(T_var, dtype='U20'))\n",
    "        A_var = list(np.array(A_var, dtype='U20'))\n",
    "                          \n",
    "W = np.concatenate((W_dev, W_test), axis=0)  \n",
    "X_s = np.concatenate((X_s_dev, X_s_test), axis=0)\n",
    "X_v = np.concatenate((X_v_dev, X_v_test), axis=0)\n",
    "T = np.concatenate((T_dev, T_test), axis=0)\n",
    "Y = np.concatenate((Y_dev, Y_test), axis=0) \n",
    "A = np.concatenate((A_dev, A_test), axis=0) \n",
    "    \n",
    "print('')\n",
    "print(\"Operation time (min): \" , (time.process_time()-t)/60)\n",
    "print('')\n",
    "print (\"W shape: \" + str(W.shape))\n",
    "print (\"X_s shape: \" + str(X_s.shape))\n",
    "print (\"X_v shape: \" + str(X_v.shape))\n",
    "print (\"T shape: \" + str(T.shape))\n",
    "print (\"A shape: \" + str(A.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W = DataFrame(data=W, columns=W_var)\n",
    "df_X_s_parameters = DataFrame(data=X_s, columns=X_s_var)\n",
    "df_X_v_parameters = DataFrame(data=X_v, columns=X_v_var)\n",
    "df_T = DataFrame(data=T, columns=T_var)\n",
    "df_Y = DataFrame(data=Y, columns=['RUL'])\n",
    "df_A = DataFrame(data=A, columns=A_var)\n",
    "df_all_units=pd.concat([df_W,df_A,df_X_s_parameters,df_X_v_parameters,df_T,df_Y],axis=1)\n",
    "def prepare_data(dataframe,list_of_columns_to_drop):\n",
    "    dataframe=dataframe.drop(columns=list_of_columns_to_drop)\n",
    "    return dataframe\n",
    "def data_test_train_splitter(DataFrame,train_idx,test_idx,list_of_columns_to_drop):\n",
    "    train_df=prepare_data(DataFrame.loc[DataFrame[\"unit\"].isin(train_idx)],list_of_columns_to_drop)\n",
    "    test_df=prepare_data(DataFrame.loc[DataFrame[\"unit\"].isin(test_idx)],list_of_columns_to_drop)\n",
    "    return (train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T2</th>\n",
       "      <th>unit</th>\n",
       "      <th>cycle</th>\n",
       "      <th>Fc</th>\n",
       "      <th>hs</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>...</th>\n",
       "      <th>W48</th>\n",
       "      <th>W50</th>\n",
       "      <th>SmFan</th>\n",
       "      <th>SmLPC</th>\n",
       "      <th>SmHPC</th>\n",
       "      <th>phi</th>\n",
       "      <th>HPT_eff_mod</th>\n",
       "      <th>LPT_eff_mod</th>\n",
       "      <th>LPT_flow_mod</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2839273</th>\n",
       "      <td>503.721197</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.809026</td>\n",
       "      <td>1445.734091</td>\n",
       "      <td>1831.247582</td>\n",
       "      <td>1234.668125</td>\n",
       "      <td>16.004249</td>\n",
       "      <td>...</td>\n",
       "      <td>219.132512</td>\n",
       "      <td>231.882086</td>\n",
       "      <td>16.755149</td>\n",
       "      <td>9.826825</td>\n",
       "      <td>25.205333</td>\n",
       "      <td>42.167109</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839274</th>\n",
       "      <td>503.699197</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.884699</td>\n",
       "      <td>1445.729730</td>\n",
       "      <td>1830.328855</td>\n",
       "      <td>1233.775609</td>\n",
       "      <td>16.004450</td>\n",
       "      <td>...</td>\n",
       "      <td>219.145580</td>\n",
       "      <td>231.898897</td>\n",
       "      <td>16.747632</td>\n",
       "      <td>9.812432</td>\n",
       "      <td>25.243954</td>\n",
       "      <td>42.128645</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839275</th>\n",
       "      <td>503.682048</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.850026</td>\n",
       "      <td>1445.623942</td>\n",
       "      <td>1830.356293</td>\n",
       "      <td>1233.817761</td>\n",
       "      <td>15.999599</td>\n",
       "      <td>...</td>\n",
       "      <td>219.061447</td>\n",
       "      <td>231.810175</td>\n",
       "      <td>16.751288</td>\n",
       "      <td>9.804538</td>\n",
       "      <td>25.239399</td>\n",
       "      <td>42.133718</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839276</th>\n",
       "      <td>503.653483</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.817989</td>\n",
       "      <td>1445.560836</td>\n",
       "      <td>1830.231656</td>\n",
       "      <td>1233.717364</td>\n",
       "      <td>15.995012</td>\n",
       "      <td>...</td>\n",
       "      <td>219.009495</td>\n",
       "      <td>231.755178</td>\n",
       "      <td>16.751436</td>\n",
       "      <td>9.806258</td>\n",
       "      <td>25.240772</td>\n",
       "      <td>42.130588</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839277</th>\n",
       "      <td>503.645699</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.803554</td>\n",
       "      <td>1445.513032</td>\n",
       "      <td>1830.195152</td>\n",
       "      <td>1233.683058</td>\n",
       "      <td>15.992129</td>\n",
       "      <td>...</td>\n",
       "      <td>218.963946</td>\n",
       "      <td>231.706993</td>\n",
       "      <td>16.754685</td>\n",
       "      <td>9.804464</td>\n",
       "      <td>25.240621</td>\n",
       "      <td>42.130812</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604563</th>\n",
       "      <td>502.759104</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>555.135733</td>\n",
       "      <td>1232.870547</td>\n",
       "      <td>1468.184069</td>\n",
       "      <td>1089.496435</td>\n",
       "      <td>13.519393</td>\n",
       "      <td>...</td>\n",
       "      <td>137.547188</td>\n",
       "      <td>145.687444</td>\n",
       "      <td>19.001082</td>\n",
       "      <td>6.042239</td>\n",
       "      <td>31.538067</td>\n",
       "      <td>34.080248</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>-0.017672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604564</th>\n",
       "      <td>502.756750</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>554.678067</td>\n",
       "      <td>1229.818158</td>\n",
       "      <td>1459.263290</td>\n",
       "      <td>1084.383871</td>\n",
       "      <td>13.498591</td>\n",
       "      <td>...</td>\n",
       "      <td>136.681546</td>\n",
       "      <td>144.774412</td>\n",
       "      <td>18.987135</td>\n",
       "      <td>6.006830</td>\n",
       "      <td>31.808141</td>\n",
       "      <td>33.798049</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>-0.017672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604565</th>\n",
       "      <td>502.770538</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>554.157953</td>\n",
       "      <td>1227.944149</td>\n",
       "      <td>1458.169108</td>\n",
       "      <td>1085.181024</td>\n",
       "      <td>13.476994</td>\n",
       "      <td>...</td>\n",
       "      <td>135.983569</td>\n",
       "      <td>144.034985</td>\n",
       "      <td>18.984340</td>\n",
       "      <td>6.002011</td>\n",
       "      <td>31.754596</td>\n",
       "      <td>33.824353</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>-0.017672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604566</th>\n",
       "      <td>502.800780</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>553.690880</td>\n",
       "      <td>1224.529086</td>\n",
       "      <td>1448.463089</td>\n",
       "      <td>1079.716205</td>\n",
       "      <td>13.456842</td>\n",
       "      <td>...</td>\n",
       "      <td>135.009254</td>\n",
       "      <td>143.009394</td>\n",
       "      <td>18.971948</td>\n",
       "      <td>5.944589</td>\n",
       "      <td>32.050241</td>\n",
       "      <td>33.521120</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>-0.017672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604567</th>\n",
       "      <td>502.798037</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>553.051454</td>\n",
       "      <td>1222.073430</td>\n",
       "      <td>1446.447510</td>\n",
       "      <td>1080.229515</td>\n",
       "      <td>13.429019</td>\n",
       "      <td>...</td>\n",
       "      <td>134.121620</td>\n",
       "      <td>142.064540</td>\n",
       "      <td>18.963461</td>\n",
       "      <td>5.930233</td>\n",
       "      <td>32.019400</td>\n",
       "      <td>33.530896</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>-0.017672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>765295 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 T2  unit  cycle   Fc   hs         T24          T30  \\\n",
       "2839273  503.721197  16.0    1.0  3.0  1.0  602.809026  1445.734091   \n",
       "2839274  503.699197  16.0    1.0  3.0  1.0  602.884699  1445.729730   \n",
       "2839275  503.682048  16.0    1.0  3.0  1.0  602.850026  1445.623942   \n",
       "2839276  503.653483  16.0    1.0  3.0  1.0  602.817989  1445.560836   \n",
       "2839277  503.645699  16.0    1.0  3.0  1.0  602.803554  1445.513032   \n",
       "...             ...   ...    ...  ...  ...         ...          ...   \n",
       "3604563  502.759104  16.0   63.0  3.0  0.0  555.135733  1232.870547   \n",
       "3604564  502.756750  16.0   63.0  3.0  0.0  554.678067  1229.818158   \n",
       "3604565  502.770538  16.0   63.0  3.0  0.0  554.157953  1227.944149   \n",
       "3604566  502.800780  16.0   63.0  3.0  0.0  553.690880  1224.529086   \n",
       "3604567  502.798037  16.0   63.0  3.0  0.0  553.051454  1222.073430   \n",
       "\n",
       "                 T48          T50        P15  ...         W48         W50  \\\n",
       "2839273  1831.247582  1234.668125  16.004249  ...  219.132512  231.882086   \n",
       "2839274  1830.328855  1233.775609  16.004450  ...  219.145580  231.898897   \n",
       "2839275  1830.356293  1233.817761  15.999599  ...  219.061447  231.810175   \n",
       "2839276  1830.231656  1233.717364  15.995012  ...  219.009495  231.755178   \n",
       "2839277  1830.195152  1233.683058  15.992129  ...  218.963946  231.706993   \n",
       "...              ...          ...        ...  ...         ...         ...   \n",
       "3604563  1468.184069  1089.496435  13.519393  ...  137.547188  145.687444   \n",
       "3604564  1459.263290  1084.383871  13.498591  ...  136.681546  144.774412   \n",
       "3604565  1458.169108  1085.181024  13.476994  ...  135.983569  144.034985   \n",
       "3604566  1448.463089  1079.716205  13.456842  ...  135.009254  143.009394   \n",
       "3604567  1446.447510  1080.229515  13.429019  ...  134.121620  142.064540   \n",
       "\n",
       "             SmFan     SmLPC      SmHPC        phi  HPT_eff_mod  LPT_eff_mod  \\\n",
       "2839273  16.755149  9.826825  25.205333  42.167109    -0.000748    -0.000499   \n",
       "2839274  16.747632  9.812432  25.243954  42.128645    -0.000748    -0.000499   \n",
       "2839275  16.751288  9.804538  25.239399  42.133718    -0.000748    -0.000499   \n",
       "2839276  16.751436  9.806258  25.240772  42.130588    -0.000748    -0.000499   \n",
       "2839277  16.754685  9.804464  25.240621  42.130812    -0.000748    -0.000499   \n",
       "...            ...       ...        ...        ...          ...          ...   \n",
       "3604563  19.001082  6.042239  31.538067  34.080248    -0.008968    -0.005504   \n",
       "3604564  18.987135  6.006830  31.808141  33.798049    -0.008968    -0.005504   \n",
       "3604565  18.984340  6.002011  31.754596  33.824353    -0.008968    -0.005504   \n",
       "3604566  18.971948  5.944589  32.050241  33.521120    -0.008968    -0.005504   \n",
       "3604567  18.963461  5.930233  32.019400  33.530896    -0.008968    -0.005504   \n",
       "\n",
       "         LPT_flow_mod  RUL  \n",
       "2839273     -0.000470   62  \n",
       "2839274     -0.000470   62  \n",
       "2839275     -0.000470   62  \n",
       "2839276     -0.000470   62  \n",
       "2839277     -0.000470   62  \n",
       "...               ...  ...  \n",
       "3604563     -0.017672    0  \n",
       "3604564     -0.017672    0  \n",
       "3604565     -0.017672    0  \n",
       "3604566     -0.017672    0  \n",
       "3604567     -0.017672    0  \n",
       "\n",
       "[765295 rows x 37 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx=[16]\n",
    "test_idx=[14,11,15]\n",
    "# list_of_columns_to_drop=[\"alt\",\"Mach\",\"TRA\",'fan_eff_mod','fan_flow_mod','LPC_eff_mod','HPC_eff_mod','HPC_flow_mod','HPT_flow_mod','LPT_eff_mod','LPT_flow_mod']\n",
    "\n",
    "list_of_columns_to_drop=[\"alt\",\"Mach\",\"TRA\",\"fan_eff_mod\",\"fan_flow_mod\",\"LPC_eff_mod\",\"LPC_flow_mod\",\"HPC_flow_mod\",\"HPC_eff_mod\",\"HPT_flow_mod\"]\n",
    "train,test=data_test_train_splitter(df_all_units,train_idx,test_idx,list_of_columns_to_drop)\n",
    "# train.drop(columns=[\"unit\",\"Fc\"])\n",
    "# test.drop(columns=[\"unit\",\"Fc\"])\n",
    "train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T2</th>\n",
       "      <th>unit</th>\n",
       "      <th>cycle</th>\n",
       "      <th>Fc</th>\n",
       "      <th>hs</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>...</th>\n",
       "      <th>W48</th>\n",
       "      <th>W50</th>\n",
       "      <th>SmFan</th>\n",
       "      <th>SmLPC</th>\n",
       "      <th>SmHPC</th>\n",
       "      <th>phi</th>\n",
       "      <th>HPT_eff_mod</th>\n",
       "      <th>LPT_eff_mod</th>\n",
       "      <th>LPT_flow_mod</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5263447</th>\n",
       "      <td>503.176696</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>601.369822</td>\n",
       "      <td>1441.086963</td>\n",
       "      <td>1822.407728</td>\n",
       "      <td>1230.069061</td>\n",
       "      <td>15.900837</td>\n",
       "      <td>...</td>\n",
       "      <td>217.085529</td>\n",
       "      <td>229.722454</td>\n",
       "      <td>16.745510</td>\n",
       "      <td>9.812495</td>\n",
       "      <td>25.345244</td>\n",
       "      <td>41.971419</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263448</th>\n",
       "      <td>503.192949</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>601.381211</td>\n",
       "      <td>1441.055436</td>\n",
       "      <td>1822.376094</td>\n",
       "      <td>1230.025551</td>\n",
       "      <td>15.900690</td>\n",
       "      <td>...</td>\n",
       "      <td>217.058720</td>\n",
       "      <td>229.694212</td>\n",
       "      <td>16.751997</td>\n",
       "      <td>9.806257</td>\n",
       "      <td>25.346932</td>\n",
       "      <td>41.971470</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263449</th>\n",
       "      <td>503.203187</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>601.392126</td>\n",
       "      <td>1441.063188</td>\n",
       "      <td>1822.350721</td>\n",
       "      <td>1229.965758</td>\n",
       "      <td>15.899810</td>\n",
       "      <td>...</td>\n",
       "      <td>217.043190</td>\n",
       "      <td>229.677650</td>\n",
       "      <td>16.758975</td>\n",
       "      <td>9.804009</td>\n",
       "      <td>25.348326</td>\n",
       "      <td>41.969940</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263450</th>\n",
       "      <td>503.158580</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>601.348485</td>\n",
       "      <td>1440.964145</td>\n",
       "      <td>1822.141800</td>\n",
       "      <td>1229.809741</td>\n",
       "      <td>15.894349</td>\n",
       "      <td>...</td>\n",
       "      <td>216.981145</td>\n",
       "      <td>229.612403</td>\n",
       "      <td>16.755378</td>\n",
       "      <td>9.803649</td>\n",
       "      <td>25.352080</td>\n",
       "      <td>41.964794</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5263451</th>\n",
       "      <td>503.105629</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>601.285695</td>\n",
       "      <td>1440.852510</td>\n",
       "      <td>1822.019760</td>\n",
       "      <td>1229.732630</td>\n",
       "      <td>15.886351</td>\n",
       "      <td>...</td>\n",
       "      <td>216.890123</td>\n",
       "      <td>229.516137</td>\n",
       "      <td>16.753262</td>\n",
       "      <td>9.806697</td>\n",
       "      <td>25.351024</td>\n",
       "      <td>41.963540</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517185</th>\n",
       "      <td>497.040848</td>\n",
       "      <td>15.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544.098139</td>\n",
       "      <td>1203.357895</td>\n",
       "      <td>1421.337905</td>\n",
       "      <td>1077.482738</td>\n",
       "      <td>12.837452</td>\n",
       "      <td>...</td>\n",
       "      <td>127.892362</td>\n",
       "      <td>135.472311</td>\n",
       "      <td>16.461637</td>\n",
       "      <td>6.788187</td>\n",
       "      <td>32.223755</td>\n",
       "      <td>33.147993</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.010910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517186</th>\n",
       "      <td>497.035821</td>\n",
       "      <td>15.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544.035020</td>\n",
       "      <td>1203.472392</td>\n",
       "      <td>1422.772517</td>\n",
       "      <td>1078.697718</td>\n",
       "      <td>12.835965</td>\n",
       "      <td>...</td>\n",
       "      <td>127.892061</td>\n",
       "      <td>135.471205</td>\n",
       "      <td>16.451330</td>\n",
       "      <td>6.803624</td>\n",
       "      <td>32.156314</td>\n",
       "      <td>33.208375</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.010910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517187</th>\n",
       "      <td>497.043961</td>\n",
       "      <td>15.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544.055787</td>\n",
       "      <td>1203.507712</td>\n",
       "      <td>1422.445115</td>\n",
       "      <td>1078.410015</td>\n",
       "      <td>12.838463</td>\n",
       "      <td>...</td>\n",
       "      <td>127.930662</td>\n",
       "      <td>135.511892</td>\n",
       "      <td>16.444379</td>\n",
       "      <td>6.810375</td>\n",
       "      <td>32.172889</td>\n",
       "      <td>33.191835</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.010910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517188</th>\n",
       "      <td>497.047255</td>\n",
       "      <td>15.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544.060142</td>\n",
       "      <td>1203.529870</td>\n",
       "      <td>1422.576000</td>\n",
       "      <td>1078.539340</td>\n",
       "      <td>12.839843</td>\n",
       "      <td>...</td>\n",
       "      <td>127.943031</td>\n",
       "      <td>135.524933</td>\n",
       "      <td>16.435526</td>\n",
       "      <td>6.810105</td>\n",
       "      <td>32.168495</td>\n",
       "      <td>33.197271</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.010910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6517189</th>\n",
       "      <td>497.049793</td>\n",
       "      <td>15.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544.066645</td>\n",
       "      <td>1203.566865</td>\n",
       "      <td>1422.622803</td>\n",
       "      <td>1078.589932</td>\n",
       "      <td>12.841603</td>\n",
       "      <td>...</td>\n",
       "      <td>127.967373</td>\n",
       "      <td>135.550693</td>\n",
       "      <td>16.424305</td>\n",
       "      <td>6.812680</td>\n",
       "      <td>32.167040</td>\n",
       "      <td>33.197768</td>\n",
       "      <td>-0.010829</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>-0.010910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1253743 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 T2  unit  cycle   Fc   hs         T24          T30  \\\n",
       "5263447  503.176696  11.0    1.0  3.0  1.0  601.369822  1441.086963   \n",
       "5263448  503.192949  11.0    1.0  3.0  1.0  601.381211  1441.055436   \n",
       "5263449  503.203187  11.0    1.0  3.0  1.0  601.392126  1441.063188   \n",
       "5263450  503.158580  11.0    1.0  3.0  1.0  601.348485  1440.964145   \n",
       "5263451  503.105629  11.0    1.0  3.0  1.0  601.285695  1440.852510   \n",
       "...             ...   ...    ...  ...  ...         ...          ...   \n",
       "6517185  497.040848  15.0   67.0  2.0  0.0  544.098139  1203.357895   \n",
       "6517186  497.035821  15.0   67.0  2.0  0.0  544.035020  1203.472392   \n",
       "6517187  497.043961  15.0   67.0  2.0  0.0  544.055787  1203.507712   \n",
       "6517188  497.047255  15.0   67.0  2.0  0.0  544.060142  1203.529870   \n",
       "6517189  497.049793  15.0   67.0  2.0  0.0  544.066645  1203.566865   \n",
       "\n",
       "                 T48          T50        P15  ...         W48         W50  \\\n",
       "5263447  1822.407728  1230.069061  15.900837  ...  217.085529  229.722454   \n",
       "5263448  1822.376094  1230.025551  15.900690  ...  217.058720  229.694212   \n",
       "5263449  1822.350721  1229.965758  15.899810  ...  217.043190  229.677650   \n",
       "5263450  1822.141800  1229.809741  15.894349  ...  216.981145  229.612403   \n",
       "5263451  1822.019760  1229.732630  15.886351  ...  216.890123  229.516137   \n",
       "...              ...          ...        ...  ...         ...         ...   \n",
       "6517185  1421.337905  1077.482738  12.837452  ...  127.892362  135.472311   \n",
       "6517186  1422.772517  1078.697718  12.835965  ...  127.892061  135.471205   \n",
       "6517187  1422.445115  1078.410015  12.838463  ...  127.930662  135.511892   \n",
       "6517188  1422.576000  1078.539340  12.839843  ...  127.943031  135.524933   \n",
       "6517189  1422.622803  1078.589932  12.841603  ...  127.967373  135.550693   \n",
       "\n",
       "             SmFan     SmLPC      SmHPC        phi  HPT_eff_mod  LPT_eff_mod  \\\n",
       "5263447  16.745510  9.812495  25.345244  41.971419    -0.000698    -0.000294   \n",
       "5263448  16.751997  9.806257  25.346932  41.971470    -0.000698    -0.000294   \n",
       "5263449  16.758975  9.804009  25.348326  41.969940    -0.000698    -0.000294   \n",
       "5263450  16.755378  9.803649  25.352080  41.964794    -0.000698    -0.000294   \n",
       "5263451  16.753262  9.806697  25.351024  41.963540    -0.000698    -0.000294   \n",
       "...            ...       ...        ...        ...          ...          ...   \n",
       "6517185  16.461637  6.788187  32.223755  33.147993    -0.010829    -0.011415   \n",
       "6517186  16.451330  6.803624  32.156314  33.208375    -0.010829    -0.011415   \n",
       "6517187  16.444379  6.810375  32.172889  33.191835    -0.010829    -0.011415   \n",
       "6517188  16.435526  6.810105  32.168495  33.197271    -0.010829    -0.011415   \n",
       "6517189  16.424305  6.812680  32.167040  33.197768    -0.010829    -0.011415   \n",
       "\n",
       "         LPT_flow_mod  RUL  \n",
       "5263447     -0.000493   58  \n",
       "5263448     -0.000493   58  \n",
       "5263449     -0.000493   58  \n",
       "5263450     -0.000493   58  \n",
       "5263451     -0.000493   58  \n",
       "...               ...  ...  \n",
       "6517185     -0.010910    0  \n",
       "6517186     -0.010910    0  \n",
       "6517187     -0.010910    0  \n",
       "6517188     -0.010910    0  \n",
       "6517189     -0.010910    0  \n",
       "\n",
       "[1253743 rows x 37 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCALING TRAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming df is your Pandas DataFrame\n",
    "# Separate the 'RUL' column from the rest of the data\n",
    "rul_values= train['RUL'].values\n",
    "features = train.drop(columns=['RUL'])\n",
    "\n",
    "features.columns = features.columns.astype(str)\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your features and transform them\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "\n",
    "# Concatenate the scaled features with the 'RUL' column\n",
    "scaled_df['RUL'] = rul_values\n",
    "\n",
    "# Now, scaled_df contains the scaled features with the 'RUL' column intact\n",
    "train=scaled_df\n",
    "train=train.drop(columns=[\"unit\",\"Fc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaling test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_values= test['RUL'].values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "rul_values= test['RUL'].values\n",
    "features = test\n",
    "\n",
    "features.columns = features.columns.astype(str)\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your features and transform them\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "\n",
    "# Concatenate the scaled features with the 'RUL' column\n",
    "\n",
    "\n",
    "# Now, scaled_df contains the scaled features with the 'RUL' column intact\n",
    "test=scaled_df\n",
    "# test=test.drop(columns=[\"unit\",\"Fc\",\"LPC_flow_mod\"])\n",
    "test=test.drop(columns=[\"unit\",\"Fc\"])\n",
    "test[\"RUL\"]=rul_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T2</th>\n",
       "      <th>cycle</th>\n",
       "      <th>hs</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>P2</th>\n",
       "      <th>P21</th>\n",
       "      <th>...</th>\n",
       "      <th>W48</th>\n",
       "      <th>W50</th>\n",
       "      <th>SmFan</th>\n",
       "      <th>SmLPC</th>\n",
       "      <th>SmHPC</th>\n",
       "      <th>phi</th>\n",
       "      <th>HPT_eff_mod</th>\n",
       "      <th>LPT_eff_mod</th>\n",
       "      <th>LPT_flow_mod</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.293728</td>\n",
       "      <td>-1.660380</td>\n",
       "      <td>1.376490</td>\n",
       "      <td>2.011258</td>\n",
       "      <td>1.948641</td>\n",
       "      <td>1.668739</td>\n",
       "      <td>2.096150</td>\n",
       "      <td>1.908224</td>\n",
       "      <td>1.463237</td>\n",
       "      <td>1.908224</td>\n",
       "      <td>...</td>\n",
       "      <td>2.360849</td>\n",
       "      <td>2.360619</td>\n",
       "      <td>-1.710277</td>\n",
       "      <td>1.859079</td>\n",
       "      <td>-1.254824</td>\n",
       "      <td>1.426255</td>\n",
       "      <td>0.816670</td>\n",
       "      <td>0.775131</td>\n",
       "      <td>0.817560</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.294732</td>\n",
       "      <td>-1.660380</td>\n",
       "      <td>1.376490</td>\n",
       "      <td>2.011886</td>\n",
       "      <td>1.948098</td>\n",
       "      <td>1.668435</td>\n",
       "      <td>2.095330</td>\n",
       "      <td>1.908156</td>\n",
       "      <td>1.463442</td>\n",
       "      <td>1.908156</td>\n",
       "      <td>...</td>\n",
       "      <td>2.359885</td>\n",
       "      <td>2.359658</td>\n",
       "      <td>-1.705908</td>\n",
       "      <td>1.852450</td>\n",
       "      <td>-1.254025</td>\n",
       "      <td>1.426277</td>\n",
       "      <td>0.816670</td>\n",
       "      <td>0.775131</td>\n",
       "      <td>0.817560</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.295365</td>\n",
       "      <td>-1.660380</td>\n",
       "      <td>1.376490</td>\n",
       "      <td>2.012488</td>\n",
       "      <td>1.948232</td>\n",
       "      <td>1.668191</td>\n",
       "      <td>2.094203</td>\n",
       "      <td>1.907752</td>\n",
       "      <td>1.463137</td>\n",
       "      <td>1.907752</td>\n",
       "      <td>...</td>\n",
       "      <td>2.359326</td>\n",
       "      <td>2.359094</td>\n",
       "      <td>-1.701208</td>\n",
       "      <td>1.850062</td>\n",
       "      <td>-1.253366</td>\n",
       "      <td>1.425632</td>\n",
       "      <td>0.816670</td>\n",
       "      <td>0.775131</td>\n",
       "      <td>0.817560</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.292609</td>\n",
       "      <td>-1.660380</td>\n",
       "      <td>1.376490</td>\n",
       "      <td>2.010080</td>\n",
       "      <td>1.946527</td>\n",
       "      <td>1.666185</td>\n",
       "      <td>2.091261</td>\n",
       "      <td>1.905243</td>\n",
       "      <td>1.460713</td>\n",
       "      <td>1.905243</td>\n",
       "      <td>...</td>\n",
       "      <td>2.357094</td>\n",
       "      <td>2.356873</td>\n",
       "      <td>-1.703630</td>\n",
       "      <td>1.849680</td>\n",
       "      <td>-1.251590</td>\n",
       "      <td>1.423463</td>\n",
       "      <td>0.816670</td>\n",
       "      <td>0.775131</td>\n",
       "      <td>0.817560</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.289338</td>\n",
       "      <td>-1.660380</td>\n",
       "      <td>1.376490</td>\n",
       "      <td>2.006616</td>\n",
       "      <td>1.944606</td>\n",
       "      <td>1.665013</td>\n",
       "      <td>2.089807</td>\n",
       "      <td>1.901569</td>\n",
       "      <td>1.457381</td>\n",
       "      <td>1.901569</td>\n",
       "      <td>...</td>\n",
       "      <td>2.353819</td>\n",
       "      <td>2.353597</td>\n",
       "      <td>-1.705056</td>\n",
       "      <td>1.852918</td>\n",
       "      <td>-1.252090</td>\n",
       "      <td>1.422935</td>\n",
       "      <td>0.816670</td>\n",
       "      <td>0.775131</td>\n",
       "      <td>0.817560</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253738</th>\n",
       "      <td>0.914691</td>\n",
       "      <td>1.807534</td>\n",
       "      <td>-0.726485</td>\n",
       "      <td>-1.148779</td>\n",
       "      <td>-2.142101</td>\n",
       "      <td>-2.182194</td>\n",
       "      <td>-0.780814</td>\n",
       "      <td>0.500944</td>\n",
       "      <td>1.187487</td>\n",
       "      <td>0.500944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.848088</td>\n",
       "      <td>-0.847448</td>\n",
       "      <td>-1.901480</td>\n",
       "      <td>-1.354496</td>\n",
       "      <td>1.999531</td>\n",
       "      <td>-2.292211</td>\n",
       "      <td>-2.782562</td>\n",
       "      <td>-4.232444</td>\n",
       "      <td>-2.252511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253739</th>\n",
       "      <td>0.914381</td>\n",
       "      <td>1.807534</td>\n",
       "      <td>-0.726485</td>\n",
       "      <td>-1.152261</td>\n",
       "      <td>-2.140130</td>\n",
       "      <td>-2.168420</td>\n",
       "      <td>-0.757906</td>\n",
       "      <td>0.500261</td>\n",
       "      <td>1.187660</td>\n",
       "      <td>0.500261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.848099</td>\n",
       "      <td>-0.847486</td>\n",
       "      <td>-1.908423</td>\n",
       "      <td>-1.338093</td>\n",
       "      <td>1.967624</td>\n",
       "      <td>-2.266764</td>\n",
       "      <td>-2.782562</td>\n",
       "      <td>-4.232444</td>\n",
       "      <td>-2.252511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253740</th>\n",
       "      <td>0.914884</td>\n",
       "      <td>1.807534</td>\n",
       "      <td>-0.726485</td>\n",
       "      <td>-1.151115</td>\n",
       "      <td>-2.139523</td>\n",
       "      <td>-2.171563</td>\n",
       "      <td>-0.763331</td>\n",
       "      <td>0.501409</td>\n",
       "      <td>1.188499</td>\n",
       "      <td>0.501409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.846710</td>\n",
       "      <td>-0.846101</td>\n",
       "      <td>-1.913105</td>\n",
       "      <td>-1.330920</td>\n",
       "      <td>1.975466</td>\n",
       "      <td>-2.273735</td>\n",
       "      <td>-2.782562</td>\n",
       "      <td>-4.232444</td>\n",
       "      <td>-2.252511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253741</th>\n",
       "      <td>0.915087</td>\n",
       "      <td>1.807534</td>\n",
       "      <td>-0.726485</td>\n",
       "      <td>-1.150875</td>\n",
       "      <td>-2.139141</td>\n",
       "      <td>-2.170307</td>\n",
       "      <td>-0.760892</td>\n",
       "      <td>0.502043</td>\n",
       "      <td>1.189094</td>\n",
       "      <td>0.502043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.846265</td>\n",
       "      <td>-0.845657</td>\n",
       "      <td>-1.919067</td>\n",
       "      <td>-1.331206</td>\n",
       "      <td>1.973386</td>\n",
       "      <td>-2.271444</td>\n",
       "      <td>-2.782562</td>\n",
       "      <td>-4.232444</td>\n",
       "      <td>-2.252511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253742</th>\n",
       "      <td>0.915244</td>\n",
       "      <td>1.807534</td>\n",
       "      <td>-0.726485</td>\n",
       "      <td>-1.150516</td>\n",
       "      <td>-2.138505</td>\n",
       "      <td>-2.169857</td>\n",
       "      <td>-0.759939</td>\n",
       "      <td>0.502851</td>\n",
       "      <td>1.189761</td>\n",
       "      <td>0.502851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.845389</td>\n",
       "      <td>-0.844780</td>\n",
       "      <td>-1.926625</td>\n",
       "      <td>-1.328470</td>\n",
       "      <td>1.972698</td>\n",
       "      <td>-2.271234</td>\n",
       "      <td>-2.782562</td>\n",
       "      <td>-4.232444</td>\n",
       "      <td>-2.252511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1253743 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               T2     cycle        hs       T24       T30       T48       T50  \\\n",
       "0        1.293728 -1.660380  1.376490  2.011258  1.948641  1.668739  2.096150   \n",
       "1        1.294732 -1.660380  1.376490  2.011886  1.948098  1.668435  2.095330   \n",
       "2        1.295365 -1.660380  1.376490  2.012488  1.948232  1.668191  2.094203   \n",
       "3        1.292609 -1.660380  1.376490  2.010080  1.946527  1.666185  2.091261   \n",
       "4        1.289338 -1.660380  1.376490  2.006616  1.944606  1.665013  2.089807   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1253738  0.914691  1.807534 -0.726485 -1.148779 -2.142101 -2.182194 -0.780814   \n",
       "1253739  0.914381  1.807534 -0.726485 -1.152261 -2.140130 -2.168420 -0.757906   \n",
       "1253740  0.914884  1.807534 -0.726485 -1.151115 -2.139523 -2.171563 -0.763331   \n",
       "1253741  0.915087  1.807534 -0.726485 -1.150875 -2.139141 -2.170307 -0.760892   \n",
       "1253742  0.915244  1.807534 -0.726485 -1.150516 -2.138505 -2.169857 -0.759939   \n",
       "\n",
       "              P15        P2       P21  ...       W48       W50     SmFan  \\\n",
       "0        1.908224  1.463237  1.908224  ...  2.360849  2.360619 -1.710277   \n",
       "1        1.908156  1.463442  1.908156  ...  2.359885  2.359658 -1.705908   \n",
       "2        1.907752  1.463137  1.907752  ...  2.359326  2.359094 -1.701208   \n",
       "3        1.905243  1.460713  1.905243  ...  2.357094  2.356873 -1.703630   \n",
       "4        1.901569  1.457381  1.901569  ...  2.353819  2.353597 -1.705056   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "1253738  0.500944  1.187487  0.500944  ... -0.848088 -0.847448 -1.901480   \n",
       "1253739  0.500261  1.187660  0.500261  ... -0.848099 -0.847486 -1.908423   \n",
       "1253740  0.501409  1.188499  0.501409  ... -0.846710 -0.846101 -1.913105   \n",
       "1253741  0.502043  1.189094  0.502043  ... -0.846265 -0.845657 -1.919067   \n",
       "1253742  0.502851  1.189761  0.502851  ... -0.845389 -0.844780 -1.926625   \n",
       "\n",
       "            SmLPC     SmHPC       phi  HPT_eff_mod  LPT_eff_mod  LPT_flow_mod  \\\n",
       "0        1.859079 -1.254824  1.426255     0.816670     0.775131      0.817560   \n",
       "1        1.852450 -1.254025  1.426277     0.816670     0.775131      0.817560   \n",
       "2        1.850062 -1.253366  1.425632     0.816670     0.775131      0.817560   \n",
       "3        1.849680 -1.251590  1.423463     0.816670     0.775131      0.817560   \n",
       "4        1.852918 -1.252090  1.422935     0.816670     0.775131      0.817560   \n",
       "...           ...       ...       ...          ...          ...           ...   \n",
       "1253738 -1.354496  1.999531 -2.292211    -2.782562    -4.232444     -2.252511   \n",
       "1253739 -1.338093  1.967624 -2.266764    -2.782562    -4.232444     -2.252511   \n",
       "1253740 -1.330920  1.975466 -2.273735    -2.782562    -4.232444     -2.252511   \n",
       "1253741 -1.331206  1.973386 -2.271444    -2.782562    -4.232444     -2.252511   \n",
       "1253742 -1.328470  1.972698 -2.271234    -2.782562    -4.232444     -2.252511   \n",
       "\n",
       "         RUL  \n",
       "0         58  \n",
       "1         58  \n",
       "2         58  \n",
       "3         58  \n",
       "4         58  \n",
       "...      ...  \n",
       "1253738    0  \n",
       "1253739    0  \n",
       "1253740    0  \n",
       "1253741    0  \n",
       "1253742    0  \n",
       "\n",
       "[1253743 rows x 35 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "test_df = test\n",
    "train_df = train\n",
    "\n",
    "# Define window size\n",
    "window_size = 4\n",
    "\n",
    "# Function to create windows\n",
    "def create_windows(df, window_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df) - window_size):\n",
    "        X.append(df.iloc[i:i + window_size].drop(columns=['RUL']).values)\n",
    "        y.append(df.iloc[i + window_size]['RUL'])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# # Create windows for training and testing\n",
    "# X_train, y_train = create_windows(train_df, window_size)\n",
    "# X_test, y_test = create_windows(test_df, window_size)\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['T2', 'unit', 'cycle', 'Fc', 'hs', 'T24', 'T30', 'T48', 'T50', 'P15',\n",
       "       'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf', 'T40',\n",
       "       'P30', 'P45', 'W21', 'W22', 'W25', 'W31', 'W32', 'W48', 'W50', 'SmFan',\n",
       "       'SmLPC', 'SmHPC', 'phi', 'HPT_eff_mod', 'LPT_eff_mod', 'LPT_flow_mod',\n",
       "       'RUL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "test_df = test\n",
    "train_df = train\n",
    "window_size=4\n",
    "def create_windows(df, window_size):\n",
    "    X = []\n",
    "    y = []  \n",
    "    \n",
    "    for i in range(len(df) - window_size + 1):\n",
    "        # Check if the current window spans multiple cycles\n",
    "        if df['cycle'].iloc[i:i + window_size].nunique() == 1:\n",
    "            X.append(df.iloc[i:i + window_size].drop(columns=['RUL', 'cycle']).values)\n",
    "            y.append(df.iloc[i + window_size - 1]['RUL'])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_windows(train_df, window_size)\n",
    "X_test, y_test = create_windows(test_df, window_size)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from torch import nn\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 1.1641\n",
      "Epoch 2/4, Loss: 0.4619\n",
      "Epoch 3/4, Loss: 0.3342\n",
      "Epoch 4/4, Loss: 1.2536\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=32, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.decoder(x[:, -1, :])  # Assuming output from the last time step\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "transformer = TransformerModel(input_dim=33).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    transformer.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = transformer(batch_x)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "    # Test MSE: 105.5501\n",
    "\n",
    "# # Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE: 31.4454\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "transformer.eval()\n",
    "val_predictions = []\n",
    "val_actuals = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = transformer(batch_x)\n",
    "        val_predictions.append(outputs.squeeze().cpu().numpy())\n",
    "        val_actuals.append(batch_y.cpu().numpy())\n",
    "\n",
    "val_predictions = np.concatenate(val_predictions)\n",
    "val_actuals = np.concatenate(val_actuals)\n",
    "\n",
    "# Calculate validation MSE\n",
    "val_mse = mean_squared_error(val_actuals, val_predictions)\n",
    "print(f'test MSE: {val_mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "model_path = 'transformer_30.3492.pth'\n",
    "torch.save(transformer, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 2.4461\n",
      "Epoch 2/4, Loss: 0.3143\n",
      "Epoch 3/4, Loss: 0.4430\n",
      "Epoch 4/4, Loss: 0.1116\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=32, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model, nhead)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            x = x + layer(self.layer_norm(x))\n",
    "\n",
    "        x = self.decoder(x[:, -1, :])  # Assuming output from the last time step\n",
    "        return x\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "transformer = TransformerModel(input_dim=33).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    transformer.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = transformer(batch_x)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE: 36.8677\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "transformer.eval()\n",
    "val_predictions = []\n",
    "val_actuals = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = transformer(batch_x)\n",
    "        val_predictions.append(outputs.squeeze().cpu().numpy())\n",
    "        val_actuals.append(batch_y.cpu().numpy())\n",
    "\n",
    "val_predictions = np.concatenate(val_predictions)\n",
    "val_actuals = np.concatenate(val_actuals)\n",
    "\n",
    "# Calculate validation MSE\n",
    "val_mse = mean_squared_error(val_actuals, val_predictions)\n",
    "print(f'test MSE: {val_mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([58., 58., 58., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50.949898, 51.348984, 51.41885 , ...,  5.435289,  5.451451,\n",
       "        5.460313], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1253137"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adding FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Loss: 2.2271\n",
      "Epoch 2/4, Loss: 0.8187\n",
      "Epoch 3/4, Loss: 0.8958\n",
      "Epoch 4/4, Loss: 1.6189\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=32, d_model=64, nhead=4, num_layers=2, dropout=0.2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model, nhead)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ffn = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, 4 * d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4 * d_model, d_model),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        for i, layer in enumerate(self.transformer_layers):\n",
    "            x = x + layer(self.ffn[i](x))  # Apply Transformer layer and FFN\n",
    "\n",
    "        x = self.decoder(x[:, -1, :])  # Assuming output from the last time step\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_dim = 33  # Example input dimension\n",
    "transformer = TransformerModel(input_dim=input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming train_loader is defined somewhere\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    transformer.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = transformer(batch_x)\n",
    "        loss = criterion(outputs.squeeze(), batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test MSE: 35.2418\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "transformer.eval()\n",
    "val_predictions = []\n",
    "val_actuals = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = transformer(batch_x)\n",
    "        val_predictions.append(outputs.squeeze().cpu().numpy())\n",
    "        val_actuals.append(batch_y.cpu().numpy())\n",
    "\n",
    "val_predictions = np.concatenate(val_predictions)\n",
    "val_actuals = np.concatenate(val_actuals)\n",
    "\n",
    "# Calculate validation MSE\n",
    "val_mse = mean_squared_error(val_actuals, val_predictions)\n",
    "print(f'test MSE: {val_mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TemporalCONV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Loss: 6.3433\n",
      "Epoch [2/4], Loss: 1.5580\n",
      "Epoch [3/4], Loss: 0.8637\n",
      "Epoch [4/4], Loss: 0.6258\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(num_channels[-1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.network(x)\n",
    "        y2 = self.fc(y1[:, :, -1])\n",
    "        return y2\n",
    "\n",
    "# Training parameters\n",
    "input_size = X_train_tensor.shape[2]\n",
    "num_channels = [64, 64, 64, 64]  # List containing the number of channels for each layer\n",
    "kernel_size = 3\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 4\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TCN(input_size, num_channels, kernel_size=kernel_size, dropout=dropout).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.permute(0, 2, 1).to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}')\n",
    "\n",
    "# Evaluation\n",
    "# model.eval()\n",
    "# test_loss = 0\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in test_loader:\n",
    "#         inputs, targets = inputs.permute(0, 2, 1).to(device), targets.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs.squeeze(), targets)\n",
    "#         test_loss += loss.item()\n",
    "# print(f'Test Loss: {test_loss / len(test_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 26.8812\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.permute(0, 2, 1).to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        test_loss += loss.item()\n",
    "print(f'Test Loss: {test_loss / len(test_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_,test_=data_test_train_splitter(df_all_units,train_idx,test_idx,list_of_columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2839273    62\n",
       "2839274    62\n",
       "2839275    62\n",
       "2839276    62\n",
       "2839277    62\n",
       "           ..\n",
       "3604563     0\n",
       "3604564     0\n",
       "3604565     0\n",
       "3604566     0\n",
       "3604567     0\n",
       "Name: RUL, Length: 765295, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_[\"RUL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example RUL values (replace with your actual data)\n",
    "rul_values = val_actuals\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot RUL values as a line\n",
    "ax.plot(val_actuals, marker='', linestyle='-')\n",
    "\n",
    "ax.plot(val_predictions, marker='', linestyle='-')\n",
    "# Customize plot\n",
    "ax.set_xlabel('Instance')\n",
    "ax.set_ylabel('Remaining Useful Life (RUL)')\n",
    "ax.set_title('Remaining Useful Life (RUL) Plot')\n",
    "ax.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T2</th>\n",
       "      <th>unit</th>\n",
       "      <th>cycle</th>\n",
       "      <th>Fc</th>\n",
       "      <th>hs</th>\n",
       "      <th>T24</th>\n",
       "      <th>T30</th>\n",
       "      <th>T48</th>\n",
       "      <th>T50</th>\n",
       "      <th>P15</th>\n",
       "      <th>...</th>\n",
       "      <th>W48</th>\n",
       "      <th>W50</th>\n",
       "      <th>SmFan</th>\n",
       "      <th>SmLPC</th>\n",
       "      <th>SmHPC</th>\n",
       "      <th>phi</th>\n",
       "      <th>HPT_eff_mod</th>\n",
       "      <th>LPT_eff_mod</th>\n",
       "      <th>LPT_flow_mod</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2839273</th>\n",
       "      <td>503.721197</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.809026</td>\n",
       "      <td>1445.734091</td>\n",
       "      <td>1831.247582</td>\n",
       "      <td>1234.668125</td>\n",
       "      <td>16.004249</td>\n",
       "      <td>...</td>\n",
       "      <td>219.132512</td>\n",
       "      <td>231.882086</td>\n",
       "      <td>16.755149</td>\n",
       "      <td>9.826825</td>\n",
       "      <td>25.205333</td>\n",
       "      <td>42.167109</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839274</th>\n",
       "      <td>503.699197</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.884699</td>\n",
       "      <td>1445.729730</td>\n",
       "      <td>1830.328855</td>\n",
       "      <td>1233.775609</td>\n",
       "      <td>16.004450</td>\n",
       "      <td>...</td>\n",
       "      <td>219.145580</td>\n",
       "      <td>231.898897</td>\n",
       "      <td>16.747632</td>\n",
       "      <td>9.812432</td>\n",
       "      <td>25.243954</td>\n",
       "      <td>42.128645</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839275</th>\n",
       "      <td>503.682048</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.850026</td>\n",
       "      <td>1445.623942</td>\n",
       "      <td>1830.356293</td>\n",
       "      <td>1233.817761</td>\n",
       "      <td>15.999599</td>\n",
       "      <td>...</td>\n",
       "      <td>219.061447</td>\n",
       "      <td>231.810175</td>\n",
       "      <td>16.751288</td>\n",
       "      <td>9.804538</td>\n",
       "      <td>25.239399</td>\n",
       "      <td>42.133718</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839276</th>\n",
       "      <td>503.653483</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.817989</td>\n",
       "      <td>1445.560836</td>\n",
       "      <td>1830.231656</td>\n",
       "      <td>1233.717364</td>\n",
       "      <td>15.995012</td>\n",
       "      <td>...</td>\n",
       "      <td>219.009495</td>\n",
       "      <td>231.755178</td>\n",
       "      <td>16.751436</td>\n",
       "      <td>9.806258</td>\n",
       "      <td>25.240772</td>\n",
       "      <td>42.130588</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839277</th>\n",
       "      <td>503.645699</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>602.803554</td>\n",
       "      <td>1445.513032</td>\n",
       "      <td>1830.195152</td>\n",
       "      <td>1233.683058</td>\n",
       "      <td>15.992129</td>\n",
       "      <td>...</td>\n",
       "      <td>218.963946</td>\n",
       "      <td>231.706993</td>\n",
       "      <td>16.754685</td>\n",
       "      <td>9.804464</td>\n",
       "      <td>25.240621</td>\n",
       "      <td>42.130812</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604563</th>\n",
       "      <td>502.759104</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>555.135733</td>\n",
       "      <td>1232.870547</td>\n",
       "      <td>1468.184069</td>\n",
       "      <td>1089.496435</td>\n",
       "      <td>13.519393</td>\n",
       "      <td>...</td>\n",
       "      <td>137.547188</td>\n",
       "      <td>145.687444</td>\n",
       "      <td>19.001082</td>\n",
       "      <td>6.042239</td>\n",
       "      <td>31.538067</td>\n",
       "      <td>34.080248</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>-0.017672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604564</th>\n",
       "      <td>502.756750</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>554.678067</td>\n",
       "      <td>1229.818158</td>\n",
       "      <td>1459.263290</td>\n",
       "      <td>1084.383871</td>\n",
       "      <td>13.498591</td>\n",
       "      <td>...</td>\n",
       "      <td>136.681546</td>\n",
       "      <td>144.774412</td>\n",
       "      <td>18.987135</td>\n",
       "      <td>6.006830</td>\n",
       "      <td>31.808141</td>\n",
       "      <td>33.798049</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>-0.017672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604565</th>\n",
       "      <td>502.770538</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>554.157953</td>\n",
       "      <td>1227.944149</td>\n",
       "      <td>1458.169108</td>\n",
       "      <td>1085.181024</td>\n",
       "      <td>13.476994</td>\n",
       "      <td>...</td>\n",
       "      <td>135.983569</td>\n",
       "      <td>144.034985</td>\n",
       "      <td>18.984340</td>\n",
       "      <td>6.002011</td>\n",
       "      <td>31.754596</td>\n",
       "      <td>33.824353</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>-0.017672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604566</th>\n",
       "      <td>502.800780</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>553.690880</td>\n",
       "      <td>1224.529086</td>\n",
       "      <td>1448.463089</td>\n",
       "      <td>1079.716205</td>\n",
       "      <td>13.456842</td>\n",
       "      <td>...</td>\n",
       "      <td>135.009254</td>\n",
       "      <td>143.009394</td>\n",
       "      <td>18.971948</td>\n",
       "      <td>5.944589</td>\n",
       "      <td>32.050241</td>\n",
       "      <td>33.521120</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>-0.017672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604567</th>\n",
       "      <td>502.798037</td>\n",
       "      <td>16.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>553.051454</td>\n",
       "      <td>1222.073430</td>\n",
       "      <td>1446.447510</td>\n",
       "      <td>1080.229515</td>\n",
       "      <td>13.429019</td>\n",
       "      <td>...</td>\n",
       "      <td>134.121620</td>\n",
       "      <td>142.064540</td>\n",
       "      <td>18.963461</td>\n",
       "      <td>5.930233</td>\n",
       "      <td>32.019400</td>\n",
       "      <td>33.530896</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>-0.017672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>765295 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 T2  unit  cycle   Fc   hs         T24          T30  \\\n",
       "2839273  503.721197  16.0    1.0  3.0  1.0  602.809026  1445.734091   \n",
       "2839274  503.699197  16.0    1.0  3.0  1.0  602.884699  1445.729730   \n",
       "2839275  503.682048  16.0    1.0  3.0  1.0  602.850026  1445.623942   \n",
       "2839276  503.653483  16.0    1.0  3.0  1.0  602.817989  1445.560836   \n",
       "2839277  503.645699  16.0    1.0  3.0  1.0  602.803554  1445.513032   \n",
       "...             ...   ...    ...  ...  ...         ...          ...   \n",
       "3604563  502.759104  16.0   63.0  3.0  0.0  555.135733  1232.870547   \n",
       "3604564  502.756750  16.0   63.0  3.0  0.0  554.678067  1229.818158   \n",
       "3604565  502.770538  16.0   63.0  3.0  0.0  554.157953  1227.944149   \n",
       "3604566  502.800780  16.0   63.0  3.0  0.0  553.690880  1224.529086   \n",
       "3604567  502.798037  16.0   63.0  3.0  0.0  553.051454  1222.073430   \n",
       "\n",
       "                 T48          T50        P15  ...         W48         W50  \\\n",
       "2839273  1831.247582  1234.668125  16.004249  ...  219.132512  231.882086   \n",
       "2839274  1830.328855  1233.775609  16.004450  ...  219.145580  231.898897   \n",
       "2839275  1830.356293  1233.817761  15.999599  ...  219.061447  231.810175   \n",
       "2839276  1830.231656  1233.717364  15.995012  ...  219.009495  231.755178   \n",
       "2839277  1830.195152  1233.683058  15.992129  ...  218.963946  231.706993   \n",
       "...              ...          ...        ...  ...         ...         ...   \n",
       "3604563  1468.184069  1089.496435  13.519393  ...  137.547188  145.687444   \n",
       "3604564  1459.263290  1084.383871  13.498591  ...  136.681546  144.774412   \n",
       "3604565  1458.169108  1085.181024  13.476994  ...  135.983569  144.034985   \n",
       "3604566  1448.463089  1079.716205  13.456842  ...  135.009254  143.009394   \n",
       "3604567  1446.447510  1080.229515  13.429019  ...  134.121620  142.064540   \n",
       "\n",
       "             SmFan     SmLPC      SmHPC        phi  HPT_eff_mod  LPT_eff_mod  \\\n",
       "2839273  16.755149  9.826825  25.205333  42.167109    -0.000748    -0.000499   \n",
       "2839274  16.747632  9.812432  25.243954  42.128645    -0.000748    -0.000499   \n",
       "2839275  16.751288  9.804538  25.239399  42.133718    -0.000748    -0.000499   \n",
       "2839276  16.751436  9.806258  25.240772  42.130588    -0.000748    -0.000499   \n",
       "2839277  16.754685  9.804464  25.240621  42.130812    -0.000748    -0.000499   \n",
       "...            ...       ...        ...        ...          ...          ...   \n",
       "3604563  19.001082  6.042239  31.538067  34.080248    -0.008968    -0.005504   \n",
       "3604564  18.987135  6.006830  31.808141  33.798049    -0.008968    -0.005504   \n",
       "3604565  18.984340  6.002011  31.754596  33.824353    -0.008968    -0.005504   \n",
       "3604566  18.971948  5.944589  32.050241  33.521120    -0.008968    -0.005504   \n",
       "3604567  18.963461  5.930233  32.019400  33.530896    -0.008968    -0.005504   \n",
       "\n",
       "         LPT_flow_mod  RUL  \n",
       "2839273     -0.000470   62  \n",
       "2839274     -0.000470   62  \n",
       "2839275     -0.000470   62  \n",
       "2839276     -0.000470   62  \n",
       "2839277     -0.000470   62  \n",
       "...               ...  ...  \n",
       "3604563     -0.017672    0  \n",
       "3604564     -0.017672    0  \n",
       "3604565     -0.017672    0  \n",
       "3604566     -0.017672    0  \n",
       "3604567     -0.017672    0  \n",
       "\n",
       "[765295 rows x 37 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_=train_.drop(columns=[\"fan_eff_mod\",\"fan_flow_mod\",\"LPC_eff_mod\",\"LPC_flow_mod\",\"HPC_flow_mod\",\"HPC_eff_mod\",\"HPT_flow_mod\"])\n",
    "train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example RUL values (replace with your actual data)\n",
    "rul_values = val_actuals\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot RUL values as a line\n",
    "ax.plot(test_[\"HPT_flow_mod\"], marker='', linestyle='-')\n",
    "\n",
    "\n",
    "# Customize plot\n",
    "ax.set_xlabel('Instance')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('modefiers')\n",
    "ax.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to drop=[\"fan_flow_mod\",\"LPC_eff_mod\",\"LPC_flow_mod\",\"HPC_flow_mod\",\"HPC_eff_mod\",\"HPT_flow_mod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_=test_.drop(columns=[\"fan_eff_mod\",\"fan_flow_mod\",\"LPC_eff_mod\",\"LPC_flow_mod\",\"HPC_flow_mod\",\"HPC_eff_mod\",\"HPT_flow_mod\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Trainer' from 'pytorch_forecasting' (c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_forecasting\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesDataSet, TemporalFusionTransformer, Trainer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NaNLabelEncoder\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuantileLoss\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Trainer' from 'pytorch_forecasting' (c:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_forecasting\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Trainer\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "# Sample data creation\n",
    "# Assuming your data has columns: time_idx, target, and multiple features\n",
    "data = pd.DataFrame({\n",
    "    \"time_idx\": np.tile(np.arange(100), 10),\n",
    "    \"target\": np.random.randn(1000),\n",
    "    \"group\": np.repeat(np.arange(10), 100),\n",
    "    \"feature1\": np.random.randn(1000),\n",
    "    \"feature2\": np.random.randn(1000),\n",
    "    \"static_feature\": np.repeat(np.random.randint(0, 10, 10), 100)\n",
    "})\n",
    "\n",
    "# Define the dataset\n",
    "max_encoder_length = 30  # length of history\n",
    "max_prediction_length = 10  # length of prediction horizon\n",
    "\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[data.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"target\",\n",
    "    group_ids=[\"group\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # allow up to 50% missing values in history\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"group\"],\n",
    "    static_reals=[\"static_feature\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"feature1\", \"feature2\"],\n",
    "    time_varying_unknown_reals=[\"target\"],\n",
    "    target_normalizer=NaNLabelEncoder(),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# Create dataloaders for training and validation\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size)\n",
    "\n",
    "# Define the Temporal Fusion Transformer model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # QuantileLoss has 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0  # set to >0 if you have a GPU\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = tft.predict(val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data type of category group was found to be numeric - use a string type / categorified string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m max_prediction_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# length of prediction horizon\u001b[39;00m\n\u001b[0;32m     22\u001b[0m training_cutoff \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m max_prediction_length\n\u001b[1;32m---> 24\u001b[0m training \u001b[38;5;241m=\u001b[39m \u001b[43mTimeSeriesDataSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_cutoff\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_encoder_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# allow up to 50% missing values in history\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_encoder_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_prediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_prediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_prediction_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_categoricals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatic_feature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_known_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_varying_unknown_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNaNLabelEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_relative_time_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_target_scales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m validation \u001b[38;5;241m=\u001b[39m TimeSeriesDataSet\u001b[38;5;241m.\u001b[39mfrom_dataset(training, data, predict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, stop_randomization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Create dataloaders for training and validation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_forecasting\\data\\timeseries.py:426\u001b[0m, in \u001b[0;36mTimeSeriesDataSet.__init__\u001b[1;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[0;32m    423\u001b[0m     data\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# dummy - real value will be set dynamiclly in __getitem__()\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;66;03m# validate\u001b[39;00m\n\u001b[1;32m--> 426\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata index has to be unique\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;66;03m# add lags\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo legion\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_forecasting\\data\\timeseries.py:639\u001b[0m, in \u001b[0;36mTimeSeriesDataSet._validate_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m specified but not found in data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m    636\u001b[0m         name \u001b[38;5;129;01min\u001b[39;00m object_columns\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (name \u001b[38;5;129;01min\u001b[39;00m category_columns \u001b[38;5;129;01mand\u001b[39;00m data[name]\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcategories\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbifc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    638\u001b[0m     ):\n\u001b[1;32m--> 639\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    640\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData type of category \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was found to be numeric - use a string type / categorified string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    641\u001b[0m         )\n\u001b[0;32m    642\u001b[0m \u001b[38;5;66;03m# check for \".\" in column names\u001b[39;00m\n\u001b[0;32m    643\u001b[0m columns_with_dot \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns[data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[1;31mValueError\u001b[0m: Data type of category group was found to be numeric - use a string type / categorified string"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_lightning import Trainer\n",
    "# Sample data creation\n",
    "# Assuming your data has columns: time_idx, target, and multiple features\n",
    "data = pd.DataFrame({\n",
    "    \"time_idx\": np.tile(np.arange(100), 10),\n",
    "    \"target\": np.random.randn(1000),\n",
    "    \"group\": np.repeat(np.arange(10), 100),\n",
    "    \"feature1\": np.random.randn(1000),\n",
    "    \"feature2\": np.random.randn(1000),\n",
    "    \"static_feature\": np.repeat(np.random.randint(0, 10, 10), 100)\n",
    "})\n",
    "\n",
    "# Define the dataset\n",
    "max_encoder_length = 30  # length of history\n",
    "max_prediction_length = 10  # length of prediction horizon\n",
    "\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[data.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"target\",\n",
    "    group_ids=[\"group\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # allow up to 50% missing values in history\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"group\"],\n",
    "    static_reals=[\"static_feature\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"feature1\", \"feature2\"],\n",
    "    time_varying_unknown_reals=[\"target\"],\n",
    "    target_normalizer=NaNLabelEncoder(),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# Create dataloaders for training and validation\n",
    "batch_size = 64\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size)\n",
    "\n",
    "# Define the Temporal Fusion Transformer model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # QuantileLoss has 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0  # set to >0 if you have a GPU\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = tft.predict(val_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
